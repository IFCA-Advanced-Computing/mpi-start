\documentclass{emi}

\usepackage{tabularx}
\usepackage{url}

\title{mpi-start and MPI-Utils}
\author{E. Fern√°ndez}
\DocVersion{1.0.2}
\EMICompVersion{@VERSION@}
\Date{\today}

\Abstract{
mpi-start is a set of scripts to close the gap between the workload management
system of a Grid insfrastructure and the configuration of the nodes on which
MPI applications are run. The package is used to help the user to start MPI
applications on heterogeneous Grid sites.
}

\begin{document}

\input{copyright}

%% Uncomment to insert and modify the suggested sections in your document. 
\tableofcontents

\newpage

\section{Functional Description}
mpi-start is a set of scripts to close the gap between the workload management
system of a Grid insfrastructure and the configuration of the nodes on which
MPI applications are run. The package is used to help the user to start MPI
applications on heterogeneous Grid sites. 

mpi-start was originally developed in the frame of the int.eu.grid project for
the execution of MPI applications with the CrossBroker metascheduler and then
extended its use as the official way of starting MPI jobs within EGEE.
Currently, it is part of the EMI project. 

There is a trac page at \url{http://devel.ifca.es/mpi-start} with information about
the development of mpi-start.

\section{Release Notes}

\subsection{What's new}

mpi-start 1.0.5 is an update release of mpi-start 1.x series that includes minor improvements and corrections.
Information about previous versions is available at mpi-start's trac page at
\url{http://devel.ifca.es/mpi-start/versions/}.

MPI-utils is a metapackge (glite-mpi) that depends on mpi-start and a yaim
plugin for configuration. No changes are introduced in this release.

\subsection{Changes}
\subsubsection{mpi-start}
Changes for mpi-start 1.0.5 are:
\begin{itemize}
\item New parameters for process mapping that allow specifying how many
processes per core/socket/node to start.
\item Support for processor and memory affinity in Open MPI (new hook)
\item Fixed bugs: \#38, \#27, \#31, \#35, \#41. 
\item Updated documentation with new options.
\item More testing coverage. 
\end{itemize}

Check the Changelog file for a detailed list of changes.

\subsubsection{MPI-Utils}
MPI-utils is a meta package with mpi-start and the yaim configuration plugin.
Changes in the yaim version 1.1.9 are:
\begin{itemize}
\item Updated documentation for yaim configuration
\item yaim plugin does not try to create mpirun wrapper in the WN
\item yaim plugin uses current torque location (or TORQUE variable)
\item yaim plugin correctly defines the runtimeenvironment variables
\end{itemize}

Check the Changelog file for a detailed list of changes.

\subsection{Requirements}
mpi-start only requires bash compatible shell for working. mpi-start uses
several commands that are available in most unix systems: readlink, mount and
mktemp.

\subsection{Source Code}

Source code is available at mpi-start repository
(\url{https://devel.ifca.es/hg/mpi-start}) with tag \texttt{mpi-start\_R\_1\_0\_5 }
and at org.glite.yaim.mpi glite
(\url{http://jra1mw.cvs.cern.ch/cgi-bin/jra1mw.cgi/org.glite.yaim.mpi/})
repository with tag \texttt{glite-yaim-mpi\_R\_1\_1\_9\_1} 

\subsection{Known Issues}
\begin{itemize}
\item Fine grained process mapping is not supported with Slurm or Condor
schedulers.
\end{itemize}

\section{User Guide}

\subsection{Description}
mpi-start is an abstraction layer that offers a unique interface to start
parallel jobs with different execution environments implementations. It
provides support for different MPI implementations.

\subsection{Installation}

Normally users do not need to install mpi-start. However if they want to use
it in a site without an existing installation, the recommendation is to create
a tarball installation that can be transfered in the input sandbox of the job.

In order to create a tarball installation, get the source code and do the following:

\begin{small}
\begin{verbatim}
$ make tarball
\end{verbatim}
\end{small}

This will create a mpi-start-X.Y.Z.tar.gz (with X.Y.Z being the version of
mpi-start) that contains all that is needed for the execution of jobs. In your
job script unpack the tarball and set the \texttt{I2G\_MPI\_START} environment
variable to \texttt{\$PWD/bin/mpi-start}.

\subsection{Usage}

mpi-start can be controlled via environment variables or command line
switches, most configuration dependent paramenters are automatically detected
by mpi-start and do not need to be specified by the user. The following
command line will be enough to run the application with the site defaults:

\begin{small}
\begin{verbatim}
$ mpi-start application [application arguments ...]
\end{verbatim}
\end{small}

\subsection{Command Line Options}
\begin{description}
\item[-h] show help message and exit
\item[-V] show mpi-start version
\item[-t mpi\_type] use \texttt{mpi\_type} as MPI implementation
\item[-v] be verbose
\item[-vv] include debug information
\item[-vvv] include full trace
\item[-pre hook] use \texttt{hook} as pre-hook file
\item[-post hook] use \texttt{hook}  as post-hook file
\item[-pcmd cmd ] use  \texttt{cmd} as pre-command
\item[-npnode n] start $n$ processes per node
\item[-pnode] start only one process per node (equivalent to -npnode 1)
\item[-npsocket n] start $n$ processes per CPU socket 
\item[-psocket] start only one process per CPU socket (equivalent to -npsocket 1)
\item[-npcore n] start $n$ processes per core
\item[-pcore] start only one process per core (equivalent to -npcore 1)
\item[-np n] set total number of processes
\item[-i file] use \texttt{file} as standard input file
\item[-o file] use \texttt{file} as standard output file
\item[-e file] use \texttt{file} as standard error file
\item[-x VAR[=VALUE]] define variable VAR with optional VALUE for the application's environment (will not be seen by mpi-start!)
\item[-d VAR=VALUE] define variable VAR with VALUE
\item[--] optional separator for application and arguments, after this, any arguments will be considered the application to run and its arguments
\end{description}

For example, the following command line would start /bin/hostname 3 times for available node using Open MPI:
\begin{small}
\begin{verbatim}
$ mpi-start -t openmpi -npnode 3 -- /bin/hostname
\end{verbatim}
\end{small}


\subsection{Environment Variables}

Prior to version 1.0.0 mpi-start only used environment variables to control
its behavior. This is still possible, although command line arguments will
override the environment variables defined. Next table shows the complete list
of variables, with the command line options that can be used to set them:

\begin{center}
\begin{tabularx}{\textwidth}{llX}
\textbf{Variable}                      & \textbf{Cmd line}  &   \textbf{Meaning}         \\
\hline
\texttt{I2G\_MPI\_APPLICATION}         & &
The application binary to execute. \\
\hline
\texttt{I2G\_MPI\_APPLICATION\_ARGS}   & &
The command line parameters for the application \\
\hline
\texttt{I2G\_MPI\_TYPE}                & -t &
The name of the MPI implementation to use. \\
\hline
\texttt{I2G\_MPI\_PRE\_RUN\_HOOK}      & -pre &
This variable can be set to a script which must define the pre\_run\_hook function. This function will be called after the MPI support has been established and before the internal pre-run hooks. This hook can be used to prepare input data or compile the program. \\
\hline
\texttt{I2G\_MPI\_POST\_RUN\_HOOK}     & -post &
This variable can be set to a script which must define the post\_run\_hook function. This function will be called after the mpirun has finished. \\
\hline
\texttt{I2G\_MPI\_START\_VERBOSE}      & -v & 
Set to 1 to turn on the additional output.\\
\hline
\texttt{I2G\_MPI\_START\_DEBUG}        & -vv &
Set to 1 to enable debugging output \\
\hline
\texttt{I2G\_MPI\_START\_TRACE}        & -vvv &
Set to 1 to trace every operation that is performed by mpi-start \\
\hline
\texttt{I2G\_MPI\_APPLICATION\_STDIN}  & -i &
Standard input file to use. \\
\hline
\texttt{I2G\_MPI\_APPLICATION\_STDOUT} & -o & 
Standard output file to use. \\
\hline
\texttt{I2G\_MPI\_APPLICATION\_STDERR} & -e &
Standard error file to use. \\
\hline
\texttt{I2G\_MPI\_SINGLE\_PROCESS}     & -pnode
& Set it to 1 to start only one process per node. \\
\hline
\texttt{I2G\_MPI\_PER\_NODE}           & -npnode
& Number of processes to start per node. \\
\hline
\texttt{I2G\_MPI\_SINGLE\_SOCKET}      & -psocket
& Set it to 1 to start only one process per CPU socket.\\
\hline
\texttt{I2G\_MPI\_PER\_SOCKET}         & -npsocket
& Number of processes to start per CPU socket. \\
\hline
\texttt{I2G\_MPI\_SINGLE\_CORE}        & -pcore
& Set it to 1 to start only one process per core. \\
\hline
\texttt{I2G\_MPI\_PER\_CORE}           & -npcore
& Number of processes to start per core. \\
\hline
\texttt{I2G\_MPI\_NP}                  & -np
& Total number of processes to start.\\
\hline
\end{tabularx} 
\end{center}

These variables can also be set with the \texttt{-d} command line switch. The
following example shows how to set the \texttt{I2G\_MPI\_TYPE} variable to
\texttt{openmpi}:
\begin{verbatim}
mpi-start -d I2G_MPI_TYPE=openmpi
\end{verbatim}

There are also other variables that can modify the behaviour of mpi-start, but
they are described in other sections of this document. The ones dealing with
site configuration of mpi-start are documented in the Site Administrator
Section (\ref{sec:sysadmin}), and the variables dealing with the Hooks are
summarized in Section \ref{sec:hookvars}.

\subsection{Scheduler and Execution Environment Support}

mpi-start support different combinations of batch schedulers and execution
environments using plugins. The schedulers are automatically detected from the
environment and the execution environment can be selected with the
\texttt{I2G\_MPI\_TYPE} variable or the \texttt{-t} command line option. 

\begin{center}
\begin{tabularx}{\textwidth}{lX}
\multicolumn{2}{c} {\textbf{Scheduler Plugins}} \\ \hline
 \texttt{sge} & supports Grid Engine. \\ 
 \texttt{pbs} & for supporting PBS/Torque. \\
 \texttt{lsf} & supports LSF. \\
 \texttt{condor} & gives support for Condor. This plugin lacks the possibility to select how many processes per node should be run. \\
 \texttt{slurm} & for supporting Slurm. As with condor, the plugin currently lacks the processes per node support. \\
 \hline
\end{tabularx} 
\end{center}

\begin{center}
\begin{tabularx}{\textwidth}{lX}
\multicolumn{2}{c} {\textbf{Execution Environment Plugins}} \\ \hline
 \texttt{openmpi} & Open MPI \\ 
 \texttt{mpich2}  & MPICH2\\ 
 \texttt{mpich}   & MPICH \\
 \texttt{lam}     & LAM-MPI \\
 \texttt{pacx}    & PACX-MPI \\
 \texttt{dummy}   & Debugging environment, just executes application in current host. \\ 
 \hline
\end{tabularx} 
\end{center}

\section{Hooks}\label{sec:hooks}
The mpi-start Hooks Framework allow the extension of mpi-start features without changing the core functionality.  Several hooks are included in the default distribution of mpi-start for
dealing with file distribution and some MPI extensions. Site admins can check
the local hooks description while users probably are interested in developing
their own hooks for compilation.

\subsection{File Distribution Hooks}

File distribution hooks are responsible for providing a common set of files prior to the execution of the application in all the hosts involved in that execution. Two steps are taken for file distribution:
\begin{itemize}
\item Detection of shared filesystem: mpi-start will try to detect if the
current working directory is in a network file system (currently considered as
such are: nfs, gfs, afs, smb, gpfs and lustre). If the detection is positive,
the distribution of files is not performed. Detection can be totally skipped
by setting: \texttt{MPI\_START\_SHARED\_FS} to 0 or 1 (0 means that mpi-start
will try distribution in the next step, and 1 that it won't). 

\item File distribution: in this step, mpi-start copies files from the current
host to the other hosts involved in the execution. It uses the most suitable
of the available distribution methods. Distribution methods are plugins that
are detected at runtime by checking all the files with \texttt{.filedist}
extension in the mpi-start \texttt{etc} directory.
\end{itemize}

The file distribution method can be fixed by using the \texttt{I2G\_MPI\_FILE\_DIST} variable.

\subsubsection{Distribution Method Plugins}
A file distribution plugin must contain the following functions:

\begin{itemize}
\item \texttt{check\_distribution\_method()}: called during initialization to check if the distribution method is suitable. It returns a number, the lower the number it returns, the higher priority it will have. If the method is not suitable, then it should return 255 or larger.
\item \texttt{copy()} perform the actual copy of files between hosts. Files
are in a gzipped tarball pointed by the \texttt{TARBALL} variable.
\item \texttt{clean()}: clean up any files once the execution is finished.
\end{itemize}

These distribution methods are included in mpi-start:
\begin{description}
\item[ssh] uses scp to copy files, needs passwordless ssh properly configured.
\item[mpiexec] uses OSC mpiexec for copying, needs OSC mpiexec installed.
\item[cptoshared\_area] copies files to a shared area that is not the current working directory. Needs the following variables:
\begin{itemize}
\item \texttt{MPI\_SHARED\_HOME}: set to \texttt{"yes"}.
\item \texttt{MPI\_SHARED\_HOME\_PATH}: path of the shared area that will be used for execution.
\end{itemize}
\item[mpi\_mt] uses mpi\_mt for copying the files. Needs the mpi\_mt binary to
be available in all machines.
\end{description}

\subsection{Extensions Hooks}

Extension hooks are local site hooks that come in the default mpi-start
distribution. The following hooks are available:

\begin{description}
\item [Affinity] The Affinity hook is enabled by setting the
\texttt{MPI\_USE\_AFFINITY} variable to \texttt{1}. When enabled (and the
execution environment supports it), it will define the appropriate options for
setting the processor affinity.
\item [OpenMP] The OpenMP hook is enabled by setting the \texttt{MPI\_USE\_OMP} variable to \texttt{1}. When enabled it will define the \texttt{OMP\_NUM\_THREADS} environment variable to the number of processors available per mpi process. 
\item [MPItrace] MPItrace is enabled by setting the \texttt{I2G\_USE\_MPITRACE} variable to \texttt{1}. It adds to the execution the mpitrace utility, assuming it is installed at \texttt{MPITRACE\_INSTALLATION}. Once the execution is finished, it gathers and creates the output files at the first host.
\item [MARMOT] Marmot is a tool for analysing and checking MPI programs. This hook enables the use of the tool if the variable \texttt{I2G\_USE\_MARMOT} is set to \texttt{1}. It also copies the analysis output to the first host.
\item [Compiler flags] Sites that do have various compilers and support various architectures may not have proper compiler flags (\texttt{MPI\_MPIxx\_OPTS}) for all the possible combination. This plugin tries to avoid compilation errors by checking the current compiler options and change them to the default architecture of the compiler if the binaries are not generated.
\end{description}

These hooks can be completely removed by deleting the affinity.hook, openmp.hook,
mpitrace.hook, marmot.hook, or compiler.hook in the mpi-start configuration
directory.

\subsection{Local Site Hooks}

Site admins can define their own hooks by creating \texttt{.hook} files in the configuration directory of mpi-start (by default /etc/mpi-start). The file must contain one of the following functions:
\begin{itemize}

\item \texttt{pre\_run\_hook ()}: it will be executed before the user hooks and the user application gets executed.
\item \texttt{post\_run\_hook ()}: it will be executed after the user application gets executed.
\end{itemize}

\subsection{Developing User Hooks}
Users can also customize the mpi-start behavior defining their own hooks by
using the \texttt{-pre} and \texttt{-post} command line switches or by setting the \texttt{I2G\_MPI\_PRE\_RUN\_HOOK}
 and \texttt{I2G\_MPI\_POST\_RUN\_HOOK} environment variables

\begin{description}
\item[\texttt{-pre} / \texttt{I2G\_MPI\_PRE\_RUN\_HOOK}] path of the file
containing the pre-hook, in this file a function called \texttt{pre\_run\_hook()} must be available. This function will be called before the application execution. The pre-hook can be used, for example, to compile the executable itself or download data.

\item[\texttt{-post} / \texttt{I2G\_MPI\_POST\_RUN\_HOOK}] path of the file
containing the post-hook, in this file a function called \texttt{post\_run\_hook()} must be available. This function will be called once the applications finishes its execution. The post-hook can be used to analyze results or to save the results on the grid.
\end{description}

Both pre and post hooks can be in the same file. Next sections contain some hook examples

\subsubsection{Compilation}
Pre-run hook can be used for generating the binaries of the application that will be run by mpi-start. The following sample shows a hook that compiles an application using \texttt{mpicc}. It assumes that the source code is called like the application binary, but with a \texttt{.c} extension. Use of complex compilation commands like configure, make, etc is also possible. This code is only executed in the first host. The results of the compilation will be available to all hosts thanks to the file distribution mechanisms.

\begin{small}
\begin{verbatim}
#!/bin/sh

# This function will be called before the execution of MPI application
pre_run_hook () {

  # Compile the program.
  echo "Compiling ${I2G_MPI_APPLICATION}"
  mpicc $MPI_MPICC_OPTS -o ${I2G_MPI_APPLICATION} ${I2G_MPI_APPLICATION}.c
  if [ ! $? -eq 0 ]; then
    echo "Error compiling program.  Exiting..."
    return 1
  fi
  echo "Successfully compiled ${I2G_MPI_APPLICATION}"
  return 0
}
\end{verbatim}
\end{small}

\subsubsection{Input Preprocessing}
Some applications require some input preprocessing before the application gets executed. For example, gromacs has a \texttt{grompp} tool that prepares the input for the actual \texttt{mdrun} application. In the following example the \texttt{grompp} tool prepares the input for gromacs:

\begin{small}
\begin{verbatim}
#!/bin/sh

pre_run_hook()
{
   echo "pre_run_hook called"

   # Here comes the pre-mpirun actions of gromacs
   export PATH=$PATH:/$VO_COMPCHEM_SW_DIR/gromacs-3.3/bin
   grompp -v -f full -o full -c after_pr -p speptide -np 4

   return 0
}
\end{verbatim}
\end{small}

\subsubsection{Output Gathering}
Applications that write output files in each of the hosts involved in the
execution may need to fetch all those files to transfer them back to the user
once the execution is finished. The following example copies all the
\texttt{mydata.*} files to the first host. It uses the
\texttt{mpi\_start\_foreach\_host} function of mpi-start that will call the
first argument for each of the hosts passing the name of the host as
parameter. 

\begin{small}
\begin{verbatim}
# the first paramter is the name of a host in the
my_copy () {
    CMD="scp . \$1:\$PWD/mydata.*"
    echo \$CMD
}

post_run_hook () {
    echo "post_run_hook called"
    if [ "x\$MPI_START_SHARED_FS" = "x0" ] ; then
        echo "gather output from remote hosts"
        mpi_start_foreach_host my_copy
    fi
    return 0
}
\end{verbatim}
\end{small}

\subsection{Hooks Variable Summary}\label{sec:hookvars}

This section contains a summary of the variables that can modify the existing
hook behaviour. They can be set using the \texttt{-d} command line switch.

\begin{center}
\begin{tabularx}{\textwidth}{llX}
\textbf{Hook} &  \textbf{Variable}  & \textbf{Meaning}         \\
\hline
File Distribution & \texttt{MPI\_SHARED\_FS} &
If undefined, mpi-start will try to detect a shared file system in the
execution directory. If defined and equal to $1$, mpi-start will assume that the execution directory is shared between all hosts and will not try to copy files. Any other value will make mpi-start assume that the execution directory is not shared. \\ \hline
File Distribution & \texttt{I2G\_MPI\_FILE\_DIST} &
Forces the use of a specific distribution method. \\
\hline
File Distribution & \texttt{MPI\_SHARED\_HOME} &
If set to \texttt{"yes"}, mpi-start will use the path defined in
\texttt{MPI\_SHARED\_HOME\_PATH} for copying the files and executing the
application. \\
\hline
File Distribution & \texttt{MPI\_SHARED\_HOME\_PATH} &
Path to a shared directory. \\
\hline
Affinity & \texttt{MPI\_USE\_AFFINITY} & If set to $1$, enable processor
affinity hook. \\
\hline
OpenMP & \texttt{MPI\_USE\_OMP} & If set to $1$, enable Open MP hook. \\
\hline
MPItrace & \texttt{I2G\_USE\_MPITRACE} & If set to $1$, enable MPItrace hook.\\
\hline
Marmot & \texttt{I2G\_USE\_MARMOT} & If set to $1$, enable Marmot hook.\\
\hline
\end{tabularx} 
\end{center}


\section{System Administrator Guide}\label{sec:sysadmin}

\subsection{Installation}

\subsubsection{Binary Distribution}

Binary packages for mpi-start are generated in EMI using ETICS. Check their repositories for the correct package for your distribution. Once you have the repositories configured you only need to install the package using your favorite package manager:

For RedHat based distributions:
\begin{small}
\begin{verbatim}
yum install mpi-start
\end{verbatim}
\end{small}

Debian based:
\begin{small}
\begin{verbatim}
apt-get install mpi-start
\end{verbatim}
\end{small}

If you are running a gLite site, you may prefer to install the glite-mpi
meta-package that includes the yaim plugin for configuraton:
\begin{small}
\begin{verbatim}
yum install glite-mpi
\end{verbatim}
\end{small}

The nodes where the user applications will be executed (Worker Nodes) also
require a working MPI implementation, Open MPI and MPICH are recommended.
The devel packages should also be installed in order to allow user to compile their
applications. Refer to your OS repositories for the exact packages.
In the case of SL5, Open MPI (including devel packages) can be installed with the following command line:
\begin{small}
\begin{verbatim}
yum install openmpi openmpi-devel
\end{verbatim}
\end{small}

\textbf{devel packages may require also the installation of a C/C++/Fortran
compiler}. Some devel packages of the MPI packages do not include the compiler
as (e.g. gcc, gcc-gfortran, gcc-g++) dependency! They should be installed also
if you want to support the compilation of MPI applications.

\subsubsection{Source Distribution}

Source can be retrieved from the mercurial repository.

Installation is as easy as "make install". The default installation prefix is "/usr", by default it also creates files in /etc/profile.d. If a non default installation wants to be done, use the PREFIX variable in make install

\begin{small}
\begin{verbatim}
$ make install  PREFIX=/opt/mpi-start
\end{verbatim}
\end{small}

In this case, is recommendable setting the installation the environment
variable I2G\_MPI\_START to point to mpi-start script (although this is not mandatory anymore). 
\begin{small}
\begin{verbatim}
$ export I2G_MPI_START=/opt/mpi-start/bin/mpi-start
\end{verbatim}
\end{small}

\subsection{mpi-start Manual Configuration}

mpi-start is designed to auto detect most of the site configurations without any administrator intervention. The default installation will automatically detect:
\begin{itemize}
\item the batch scheduler at the site: currently PBS/Torque, SGE, LSF, Condor and Slurm are supported.
\item existence of shared file system in the job running directory
\item availability of OSC mpiexec for PBS/Torque systems
\item default mpi installations for SLC5.
\end{itemize}

If the automatic detection fails for any of these, the administrator can set the following configuration variables:

\begin{center}
\begin{tabularx}{\textwidth}{lX}
\textbf{Variable}        &              \textbf{Meaning}         \\ \hline
\texttt{MPI\_DEFAULT\_FLAVOUR} & name of the default flavour for jobs running at the site \\ \hline
\texttt{MPI\_<flavour>\_PATH} & Path of the bin and lib directories for the MPI flavour \\ \hline
\texttt{MPI\_<flavour>\_VERSION} & preferred version of the MPI flavour \\ \hline
\texttt{MPI\_<flavour>\_MPIEXEC} & Path of the MPIEXEC binary for the specific flavour \\ \hline
\texttt{MPI\_<flavour>\_MPIEXEC\_PARAMS} & Parameters for the MPIEXEC of the flavour \\ \hline
\texttt{MPI\_<flavour>\_MPIRUN} & Path of the MPIRUN binary for the specific flavour \\ \hline
\texttt{MPI\_<flavour>\_MPIRUN\_PARAMS} & Parameters for the MPIRUN of the flavour \\ \hline
\texttt{I2G\_<flavour>\_PREFIX} & Path of the MPI installation for the MPI flavour \\ \hline
\end{tabularx} 
\end{center}

A known issue with the setting of the I2G\_<flavour>\_PREFIX variable makes
them useless, please use the MPI\_<flavour>\_PATH variable instead!

If \texttt{MPI\_<flavour>\_MPIEXEC} or \texttt{MPI\_<flavour>\_MPIRUN} are not defined, mpi-start will try to use the mpiexec or mpirun that are found in the current PATH.

\subsubsection{Hooks}
Hooks may change the behavior of mpi-start and provide additional features
such as file distribution and configuration of compiler flags. Site admins can
add their own hooks via the local hook mechanism.

mpi-start includes hooks for distributing the files needed for the execution
of an application. By default it tries to find the most suitable method for
copying the files, using shared filesystems whenever they are found. However,
the filesystem detection may not work for all sites, or the shared filesystem
may be in a different location to the execution path making it impossible for
mpi-start to detect its availability. Check Section \ref{sec:hooks} for more
information. Section \ref{sec:hookvars} contains a summary of relevant
variables that may defined.

\subsection{mpi-start Yaim Configuration}

Configuration is necessary on both the CE and WNs in order to support and advertise MPI correctly. This is performed by the yaim MPI module which should be run on both types of nodes.

\subsubsection{WN Configuration}

The yaim plugin in the WN prepares the environment for the correct execution of mpi-start. Each of the MPI flavours supported by the site must be specified setting the variable \texttt{MPI\_<FLAVOUR>\_ENABLE} to \texttt{"yes"}. For example, to enable Open MPI, add the following:

\begin{small}
\begin{verbatim}
MPI_OPENMPI_ENABLE="yes"
\end{verbatim}
\end{small}

Optionally, if you are using a non OS provided MPI implementation, you can
define the location and version with \texttt{MPI\_<FLAVOUR>\_VERSION} and
\texttt{MPI\_<FLAVOUR>\_PATH}. \textbf{Do not use these variables if you are
using the OS provided MPI implementations}. For example for Open MPI version 1.3, installed at /opt/openmpi-1.3:

\begin{small}
\begin{verbatim}
MPI_OPENMPI_VERSION="1.3"
MPI_OPENMPI_PATH="/opt/openmpi-1.3/"
\end{verbatim}
\end{small}

MPI flavours that use a particular mpiexec for starting the jobs (e.g. OSC
mpiexec for PBS/Torque system) may also provide in the
\texttt{MPI\_<FLAVOUR>\_MPIEXEC} the path to the binary. \textbf{Do not use
this variable if you are not using a different mpiexec from the one provided
by the MPI implementation.}

Additionally, you may specify a default MPI flavour to use if non is selected for execution, with the \texttt{MPI\_DEFAULT\_FLAVOUR}. If no default flavour is specified, the first one defined in your site-info.def will be considered as default.

If you provide a shared filesystem for the execution of the applications, but
it is not the path where the jobs are started, then set the variable
\texttt{MPI\_SHARED\_HOME} to \texttt{"yes"} and the variable
\texttt{MPI\_SHARED\_HOME\_PATH} to the the location of the shared filesystem.
\textbf{Do not use this variable if the application starts its execution in a
shared directory (e.g. shared home), this situation should be automatically
detected}.

If you use ssh host based authentication, set the variable \texttt{MPI\_SSH\_HOST\_BASED\_AUTH} to \texttt{"yes"}.

Lastly, if your use a non default location for mpi-start, set its location
with the \texttt{MPI\_MPI\_START} variable.

The complete list of configuration variables for the WN is shown in the next table:
\begin{center}
\begin{tabularx}{\textwidth}{llX}
\textbf{Variable}     & \textbf{Mandatory} & \textbf{Description}        \\ \hline
\texttt{MPI\_<FLAVOUR>\_ENABLE}      & YES & set to \texttt{"yes"} if you want to enable the <flavour> \\ \hline 
\texttt{MPI\_<FLAVOUR>\_VERSION}     & NO  & set to the supported version of the <flavour>, usually is automatically detected \\ \hline
\texttt{MPI\_<FLAVOUR>\_PATH}        & NO  & set to the path of supported version of the <flavour>, usually is automatically detected by the yaim WN plugin \\ \hline
\texttt{MPI\_<FLAVOUR>\_MPIEXEC}     & NO  & If you are using OSC mpiexec (only in PBS/Torque sites), set this to the location of the mpiexec program, e.g. \texttt{"/usr/bin/mpiexec"} \\ \hline
\texttt{MPI\_DEFAULT\_FLAVOUR}       & NO  & Set it to the default flavour for your site, if undefined, the first defined flavour will be used \\ \hline
\texttt{MPI\_SHARED\_HOME}           & NO  & set this to \texttt{"yes"} if you have a shared home area between WNs. \\ \hline
\texttt{MPI\_SHARED\_HOME\_PATH}     & NO  & location of the shared area for execution of MPI applications \\ \hline
\texttt{MPI\_SSH\_HOST\_BASED\_AUTH} & NO  & set it to \texttt{"yes"} if you have SSH based authentication between WNs \\ \hline
\texttt{MPI\_MPI\_START}             & NO  & Location of mpi-start if not installed in standard location (\texttt{/usr/bin/mpi-start}) \\ \hline
\end{tabularx}
\end{center}

The profile for a worker node is MPI\_WN. Use it along any other profiles you may need for your WN. 
\begin{small}
\begin{verbatim}
/opt/glite/yaim/bin/yaim -c -s site-info.def  -n MPI_WN -n <other_WN_profiles>
\end{verbatim}
\end{small}

\subsubsection{CE Configuration}

As with the WN, individual flavours of MPI are enabled by setting the \texttt{MPI\_<FLAVOUR>\_ENABLE} 
associated variable to \texttt{"yes"}. The version of the MPI implementation must also
be specified with the variable \texttt{MPI\_<FLAVOUR>\_VERSION}, e.g. for configuring Open MPI version 1.3:

\begin{small}
\begin{verbatim}
MPI_OPENMPI_ENABLE="yes"
MPI_OPENMPI_VERSION="1.3"
\end{verbatim}
\end{small}

Possible flavours are:
\begin{description}
\item[OPENMPI] for Open MPI
\item[MPICH] for MPICH-1
\item[MPICH2] for MPICH-2
\item[LAM] for LAM-MPI
\end{description}

The use of shared homes should be announced also by setting the \texttt{MPI\_SHARED\_HOME} to \texttt{"yes"}.

If you are using PBS/Torque, you can set the variable
\texttt{MPI\_SUBMIT\_FILTER} to \texttt{"yes"} in order to enable the
submission of parallel jobs in your system. The submit filter assumes that
your Worker Nodes are correctly configured to publish in their status the \texttt{ncpus}
variable with the number of available slots. If that's not true in your case,
you may edit the file \texttt{/var/torque/submit\_filter} in line 71 to fit
your pbsnodes output. An example for using the \texttt{np} value is commented
out in the file.

The complete list of configuration variables for the CE is shown in the next table:
\begin{center}
\begin{tabularx}{\textwidth}{llX}
\textbf{Variable}     & \textbf{Mandatory} & \textbf{Description}        \\ \hline
\textbf{MPI\_<FLAVOUR>\_ENABLE}    & YES & set to \texttt{"yes"} if you want to enable the <flavour> \\ \hline
\textbf{MPI\_<FLAVOUR>\_VERSION}   & YES & set to the supported version of the <flavour>, usually is automatically detected \\ \hline
\textbf{MPI\_SHARED\_HOME}         & NO  & set this to \texttt{"yes"} if you have a shared home area between WNs. \\ \hline
\textbf{MPI\_SUBMIT\_FILTER}       & NO  & Set this to \texttt{"yes"} to configure the submit filter for torque batch system that enables the submission of parallel jobs. The configuration assumes that torque path is \texttt{/var/torque} or \texttt{TORQUE\_VAR\_DIR} variable if defined. \\ \hline
\end{tabularx}
\end{center}

The profile for configuring the CE is MPI\_CE. 
\begin{small}
\begin{verbatim}
/opt/glite/yaim/bin/yaim -c -s site-info.def  -n MPI_CE -n <other_ce_profiles>
\end{verbatim}
\end{small}

\textbf{Batch system and MPI}: The batch system may need extra configuration for the submission of MPI jobs.
In PBS, you may use the automatic creation of the submit filter with the
\texttt{MPI\_SUBMIT\_FILTER} variable. In the case of SGE you need to configure a parallel environment.
Check the documentation of your batch system for any further details.

\textbf{MPI\_CE and other yaim profiles}: The \texttt{MPI\_CE} profile should
be the first in the yaim configuration, otherwise the Glue variables will not
be properly defined. This restriction may be removed in future versions.


\subsubsection{Example configuration}

Here is an example configuration (with both CEs and WN variables!):

\begin{small}
\begin{verbatim}
#----------------------------------
# MPI-related configuration:
#----------------------------------
# Several MPI implementations (or "flavours") are available.
# If you do NOT want a flavour to be installed/configured, set its variable
# to "no". Else, set it to "yes" (default). If you want to use an
# already installed version of an implementation, set its "_PATH" and
# "_VERSION" variables to match your setup (examples below).
#
# NOTE 1: the CE_RUNTIMEENV will be automatically updated in the file
# functions/config_mpi, so that the CE advertises the MPI implementations
# you choose here - you do NOT have to change it manually in this file.
# It will become something like this:
#
#   CE_RUNTIMEENV="$CE_RUNTIMEENV
#              MPI_MPICH
#              MPI_MPICH2
#              MPI_OPENMPI
#              MPI_LAM"
#
# NOTE 2: it is currently NOT possible to configure multiple concurrent
# versions of the same implementations (e.g. MPICH-1.2.3 and MPICH-1.2.7)
# using YAIM. Customize "/opt/glite/yaim/functions/config_mpi" file
# to do so.

MPI_MPICH_ENABLE="yes"
MPI_MPICH_VERSION="1.2.7p1"

MPI_MPICH2_ENABLE="yes"
MPI_MPICH2_VERSION="1.0.4"

MPI_OPENMPI_ENABLE="yes"
MPI_OPENMPI_VERSION="1.1"

MPI_LAM_ENABLE="yes"
MPI_LAM_VERSION="7.1.2"

# set Open MPI as default flavour
MPI_DEFAULT_FLAVOUR=OPENMPI

#---
# Example for using an already installed version of MPI.
# Setting "_PATH" and "_VERSION" variables will prevent YAIM
# from downloading and installing the gLite-provided packages.
# Just fill in the path to its current installation (e.g. "/usr")
# and which version it is (e.g. "6.5.9").
# DO NOT USE UNLESS A NON DEFAULT LOCATION IS USED
#---
# MPI_MPICH_PATH="/opt/mpich-1.2.7p1/"
# MPI_MPICH2_PATH="/opt/mpich2-1.0.4/"

# If you do NOT provide a shared home, set $MPI_SHARED_HOME to "no" (default).
#
# MPI_SHARED_HOME="yes"

#
# If you do NOT have SSH Hostbased Authentication between your WNs, set the below
# variable to "no" (default). Else, set it to "yes".
#
# MPI_SSH_HOST_BASED_AUTH="yes"


# If you use Torque as batch system, you may want to let the yaim plugin
# configure a submit filter for you. Uncomment the following line to do so
# MPI_SUBMIT_FILTER="yes"

#
# If you provide an 'mpiexec' for MPICH or MPICH2, please state the full path to
# that file here (http://www.osc.edu/~pw/mpiexec/index.php). Else, leave empty.
#
#MPI_MPICH_MPIEXEC="/usr/bin/mpiexec"
\end{verbatim}
\end{small}

\section{Examples}

\subsection{Simple Job}

Simple job using environment variables:

\begin{small}
\begin{verbatim}
#!/bin/sh
# IMPORTANT : This example script execute a
#             non-mpi program with Open MPI
#
export I2G_MPI_APPLICATION=/bin/hostname
export I2G_MPI_TYPE=openmpi

$I2G_MPI_START
\end{verbatim}
\end{small}

Same example using command line parameters:
\begin{small}
\begin{verbatim}
mpi-start -t openmpi /bin/hostname
\end{verbatim}
\end{small}

\subsection{Job with User Hooks}
\begin{small}
\begin{verbatim}
#!/bin/sh
#
# MPI_START_SHARED_FS can be used to figure out if the current working
# is located on a shared file system or not. (1=yes, 0=no);
#
# The "mpi_start_foreach_host" function takes as parameter the name of
# another function that will be called for each host in the machine as
# first parameter.
# - For each host the callback function will be called exactly once,
#   independent how often the host appears in the machinefile.
# - The callback function will also be called for the local host.

# create the pre-run hook
cat > pre_run_hook.sh << EOF

pre_run_hook () {
    echo "pre run hook called "
    # - download data
    # - compile program

    if [ "x\$MPI_START_SHARED_FS" = "x0" ] ; then
        echo "If we need a shared file system we can return -1 to abort"
        # return -1
    fi

    return 0
}
EOF

# create the post-run hook
cat > post_run_hook.sh << EOF
# the first paramter is the name of a host in the
my_copy () {
    CMD="scp . \$1:\$PWD/mydata.1"
    echo \$CMD
    #\$CMD
    # upload data
}

post_run_hook () {
    echo "post_run_hook called"
    if [ "x\$MPI_START_SHARED_FS" = "x0" ] ; then
        echo "gather output from remote hosts"
        mpi_start_foreach_host my_copy
    fi
    return 0
}
EOF

export I2G_MPI_APPLICATION=mpi_sleep
export I2G_MPI_APPLICATION_ARGS=0
export I2G_MPI_TYPE=openmpi
export I2G_MPI_PRE_RUN_HOOK=./pre_run_hook.sh
export I2G_MPI_POST_RUN_HOOK=./post_run_hook.sh

$I2G_MPI_START

# instead of the variable definition, the following command line could be used:
# mpi-start -t openmpi -pre ./pre_run_hook.sh -post ./post_run_hook.sh mpi_sleep 0
\end{verbatim}
\end{small}

\subsection{Using mpi-start with gLite}

gLite uses the WMS software for submitting jobs to the different available resources. The WMS gets a job description in the JDL language and performs the selection and actual submission of the job into the resources on behalf of the user. The following sections describe how to submit a job using the WMS.

\subsubsection{Basic Job Submission}
Jobs are described with the JDL language. Most relevant attributes for parallel job submission are:
\begin{itemize}
\item CPUNumber: number of processes to allocate.
\item Requirements: requirements of the job, will allow to force the selection of sites with mpi-start support.
\end{itemize}

The following example shows a job that will use 6 processes and it is executed with Open MPI. The \texttt{requirements} attribute makes the WMS to select sites that publish that they support mpi-start and Open MPI.

\begin{small}
\begin{verbatim}
JobType       = "Normal";
CPUNumber     = 6;
Executable    = "starter.sh";
Arguments     = "OPENMPI hello_bin hello arguments";
InputSandbox  = {"starter.sh", "hello_bin"};
OutputSandbox = {"std.out", "std.err"};
StdOutput     = "std.out";
StdError      = "std.err";
Requirements  = member("MPI-START", other.GlueHostApplicationSoftwareRunTimeEnvironment)
                && member("OPENMPI", other.GlueHostApplicationSoftwareRunTimeEnvironment);
\end{verbatim}
\end{small}

The \texttt{Executable} attribute is a script that will invoke mpi-start with the correct options for the execution of the user's application. We propose a generic wrapper that can be used for any application and MPI flavour that gets in the \texttt{Arguments} attribute:

\begin{itemize}
\item Name of mpi-start execution environment (I2G\_MPI\_FLAVOUR variable), in the example: OPENMPI
\item Name of user binary, in the example: hello\_bin
\item Arguments for the user binary, in the example: hello arguments
\end{itemize}

This is the content of the wrapper:

\begin{small}
\begin{verbatim}
#!/bin/bash
# Pull in the arguments.
MPI_FLAVOR=$1

MPI_FLAVOR_LOWER=`echo $MPI_FLAVOR | tr '[:upper:]' '[:lower:]'`
export I2G_MPI_TYPE=$MPI_FLAVOR_LOWER

shift
export I2G_MPI_APPLICATION=$1

shift
export I2G_MPI_APPLICATION_ARGS=$*

# Touch the executable, and make sure it's executable.
touch $I2G_MPI_APPLICATION
chmod +x $I2G_MPI_APPLICATION

# Invoke mpi-start.
$I2G_MPI_START
\end{verbatim}
\end{small}

User needs to include this wrapper in the \texttt{InputSandbox} of the JDL (\texttt{starter.sh}) and set it as the \texttt{Executable} of the job. Submission is performed as any other gLite job:

\begin{small}
\begin{verbatim}
$ glite-wms-job-submit -a hello-mpi.sh

Connecting to the service https://gridwms01.ifca.es:7443/glite_wms_wmproxy_server


====================== glite-wms-job-submit Success ======================

The job has been successfully submitted to the WMProxy
Your job identifier is:

https://gridwms01.ifca.es:9000/8jG3MUNRm-ol7BqhFP5Crg

==========================================================================
\end{verbatim}
\end{small}

Once the job is finished, the output can be retrieved:

\begin{small}
\begin{verbatim}
$ glite-wms-job-output https://gridwms01.ifca.es:9000/8jG3MUNRm-ol7BqhFP5Crg

Connecting to the service https://gridwms01.ifca.es:7443/glite_wms_wmproxy_server

================================================================================

                        JOB GET OUTPUT OUTCOME

Output sandbox files for the job:
https://gridwms01.ifca.es:9000/8jG3MUNRm-ol7BqhFP5Crg
have been successfully retrieved and stored in the directory:
/gpfs/csic_projects/grid/tmp/jobOutput/enol_8jG3MUNRm-ol7BqhFP5Crg

================================================================================


$ cat /gpfs/csic_projects/grid/tmp/jobOutput/enol_8jG3MUNRm-ol7BqhFP5Crg/std.*
Hello world from gcsic054wn. Process 3 of 6
Hello world from gcsic054wn. Process 1 of 6
Hello world from gcsic054wn. Process 2 of 6
Hello world from gcsic054wn. Process 0 of 6
Hello world from gcsic055wn. Process 4 of 6
Hello world from gcsic055wn. Process 5 of 6
\end{verbatim}
\end{small}

\subsubsection{Modifying mpi-start Behavior}
mpi-start behavior can be customized by setting different environment
variables. If using the generic wrapper, one easy way of customizing mpi-start
execution is using the \texttt{Environment} attribute of the JDL. The following JDL adds debugging to
the previous example by setting the \texttt{I2G\_MPI\_START\_VERBOSE} and
\texttt{I2G\_MPI\_START\_DEBUG} variables to 1:

\begin{small}
\begin{verbatim}
JobType       = "Normal";
CPUNumber     = 6;
Executable    = "starter.sh";
Arguments     = "OPENMPI hello_bin hello arguments";
InputSandbox  = {"starter.sh", "hello_bin"};
OutputSandbox = {"std.out", "std.err"};
StdOutput     = "std.out";
StdError      = "std.err";
Requirements  = member("MPI-START", other.GlueHostApplicationSoftwareRunTimeEnvironment)
             && member("OPENMPI", other.GlueHostApplicationSoftwareRunTimeEnvironment);
Environment   = {"I2G_MPI_START_VERBOSE=1", "I2G_MPI_START_DEBUG=1"};
\end{verbatim}
\end{small}

Use of hooks is also possible using this mechanism. If the user has a file
with the mpi-start hooks called \texttt{hooks.sh}, the following JDL would add
it to the execution (notice that the file is also added in the
\texttt{InputSandbox}):

\begin{small}
\begin{verbatim}
JobType       = "Normal";
CPUNumber     = 6;
Executable    = "starter.sh";
Arguments     = "OPENMPI hello_bin hello arguments";
InputSandbox  = {"starter.sh", "hello_bin", "hooks.sh"};
OutputSandbox = {"std.out", "std.err"};
StdOutput     = "std.out";
StdError      = "std.err";
Requirements  = member("MPI-START", other.GlueHostApplicationSoftwareRunTimeEnvironment)
             && member("OPENMPI", other.GlueHostApplicationSoftwareRunTimeEnvironment);
Environment   = {"I2G_MPI_PRE_RUN_HOOK=hooks.sh", "I2G_MPI_POST_RUN_HOOK=hooks.sh"};
\end{verbatim}
\end{small}

\end{document}
