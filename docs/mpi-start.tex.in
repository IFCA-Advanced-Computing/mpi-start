\documentclass{emi}

\usepackage{tabularx}
\usepackage{url}

\title{mpi-start and MPI-Utils}
\author{E. Fern√°ndez}
\DocVersion{1.0.5}
\EMICompVersion{@VERSION@}
\Date{\today}

\Abstract{
mpi-start and MPI-Utils help grid users and administrators to close the gap between the workload management system of a Grid insfrastructure and the configuration of the nodes on which MPI applications are run. The package is used to help users to start MPI applications on heterogeneous Grid sites.
}

\begin{document}

\input{copyright}

%% Uncomment to insert and modify the suggested sections in your document. 
\tableofcontents

\newpage

\section{About}

\subsection{mpi-start}
\subsubsection{Description}
mpi-start is a set of scripts to close the gap between the workload management
system of a Grid insfrastructure and the configuration of the nodes on which
MPI applications are run. The package is used to help the user to start MPI
applications on heterogeneous Grid sites. 

mpi-start provides an abstraction layer that offers a unique interface to start parallel jobs with different execution environments implementations. It supports several different MPI implementations under different batch systems.

mpi-start was originally developed in the frame of the int.eu.grid project for
the execution of MPI applications with the CrossBroker metascheduler and then
extended its use as the official way of starting MPI jobs within EGEE.
Currently, it is part of the EMI project. 

There is a trac page at \url{http://devel.ifca.es/mpi-start} with information about the development of mpi-start.

\subsubsection{Requirements}
mpi-start only requires bash compatible shell for working. mpi-start uses several commands that are available in most unix systems: readlink, mount and mktemp.

\subsubsection{Source Code}
Source code is available at mpi-start mercurial repository (\url{https://devel.ifca.es/hg/mpi-start}). Released versions are tagged as \texttt{mpi-start\_R\_X\_Y\_Z-r}, where X.Y.Z is the mpi-start version and r the revision number for that version.

\subsection{MPI-Utils}
\subsubsection{Description}
MPI-utils is a metapackge (emi-mpi) that depends on mpi-start and a yaim plugin for easy configuration of the MPI support in grid sites.

\subsubsection{Requirements}
MPI-Utils contains a yaim plugin, therefore it needs yaim-core installation available. The plugin does not use any non-standard tools. 

\subsubsection{Source Code}
The source code of the yaim plugin is available at glite CVS (\url{http://jra1mw.cvs.cern.ch/cgi-bin/jra1mw.cgi/org.glite.yaim.mpi}) under org.glite.yaim.mpi module. Released versions are tagged with tags in the form \texttt{glite-yaim-mpi\_R\_X\_Y\_Z\_r},   where X.Y.Z is the plugin version and r the revision number for that version.

\newpage
\section{User Guide}
\subsection{Installation}

Normally users do not need to install mpi-start. However if they want to use
it in a site without an existing installation, the recommendation is to create
a tarball installation that can be transfered in the input sandbox of the job.

In order to create a tarball installation, get the source code and do the following:

\begin{small}
\begin{verbatim}
$ make tarball
\end{verbatim}
\end{small}

This will create a mpi-start-X.Y.Z.tar.gz (with X.Y.Z being the version of
mpi-start) that contains all that is needed for the execution of jobs. In your
job script unpack the tarball and set the \texttt{I2G\_MPI\_START} environment
variable to \texttt{\$PWD/bin/mpi-start}.

\subsection{Usage}

mpi-start can be controlled via environment variables or command line
switches, most configuration dependent paramenters are automatically detected
by mpi-start and do not need to be specified by the user. The following
command line will be enough to run the application with the site defaults:

\begin{small}
\begin{verbatim}
$ mpi-start application [application arguments ...]
\end{verbatim}
\end{small}

\subsection{Command Line Options}
\begin{description}
\item[-h] show help message and exit
\item[-V] show mpi-start version
\item[-t mpi\_type] use \texttt{mpi\_type} as MPI implementation
\item[-v] be verbose
\item[-vv] include debug information
\item[-vvv] include full trace
\item[-pre hook] use \texttt{hook} as pre-hook file
\item[-post hook] use \texttt{hook}  as post-hook file
\item[-pcmd cmd ] use  \texttt{cmd} as pre-command
\item[-npnode n] start $n$ processes per node
\item[-pnode] start only one process per node (equivalent to -npnode 1)
\item[-npsocket n] start $n$ processes per CPU socket 
\item[-psocket] start only one process per CPU socket (equivalent to -npsocket 1)
\item[-npcore n] start $n$ processes per core
\item[-pcore] start only one process per core (equivalent to -npcore 1)
\item[-np n] set total number of processes
\item[-i file] use \texttt{file} as standard input file
\item[-o file] use \texttt{file} as standard output file
\item[-e file] use \texttt{file} as standard error file
\item[-x VAR[=VALUE]] define variable VAR with optional VALUE for the application's environment (will not be seen by mpi-start!)
\item[-d VAR=VALUE] define variable VAR with VALUE
\item[--] optional separator for application and arguments, after this, any arguments will be considered the application to run and its arguments
\end{description}

For example, the following command line would start /bin/hostname 3 times for available node using Open MPI:
\begin{small}
\begin{verbatim}
$ mpi-start -t openmpi -npnode 3 -- /bin/hostname
\end{verbatim}
\end{small}

\subsection{Environment Variables}

Prior to version 1.0.0 mpi-start only used environment variables to control
its behavior. This is still possible, although command line arguments will
override the environment variables defined. Next table shows the complete list
of variables, with the command line options that can be used to set them:

\begin{center}
\begin{tabularx}{\textwidth}{llX}
\textbf{Variable}                      & \textbf{Cmd line}  &   \textbf{Meaning}         \\
\hline
\texttt{I2G\_MPI\_APPLICATION}         & &
The application binary to execute. \\
\hline
\texttt{I2G\_MPI\_APPLICATION\_ARGS}   & &
The command line parameters for the application \\
\hline
\texttt{I2G\_MPI\_TYPE}                & -t &
The name of the MPI implementation to use. \\
\hline
\texttt{I2G\_MPI\_PRE\_RUN\_HOOK}      & -pre &
This variable can be set to a script which must define the pre\_run\_hook function. This function will be called after the MPI support has been established and before the internal pre-run hooks. This hook can be used to prepare input data or compile the program. \\
\hline
\texttt{I2G\_MPI\_POST\_RUN\_HOOK}     & -post &
This variable can be set to a script which must define the post\_run\_hook function. This function will be called after the mpirun has finished. \\
\hline
\texttt{I2G\_MPI\_START\_VERBOSE}      & -v & 
Set to 1 to turn on the additional output.\\
\hline
\texttt{I2G\_MPI\_START\_DEBUG}        & -vv &
Set to 1 to enable debugging output \\
\hline
\texttt{I2G\_MPI\_START\_TRACE}        & -vvv &
Set to 1 to trace every operation that is performed by mpi-start \\
\hline
\texttt{I2G\_MPI\_APPLICATION\_STDIN}  & -i &
Standard input file to use. \\
\hline
\texttt{I2G\_MPI\_APPLICATION\_STDOUT} & -o & 
Standard output file to use. \\
\hline
\texttt{I2G\_MPI\_APPLICATION\_STDERR} & -e &
Standard error file to use. \\
\hline
\texttt{I2G\_MPI\_SINGLE\_PROCESS}     & -pnode
& Set it to 1 to start only one process per node. \\
\hline
\texttt{I2G\_MPI\_PER\_NODE}           & -npnode
& Number of processes to start per node. \\
\hline
\texttt{I2G\_MPI\_SINGLE\_SOCKET}      & -psocket
& Set it to 1 to start only one process per CPU socket.\\
\hline
\texttt{I2G\_MPI\_PER\_SOCKET}         & -npsocket
& Number of processes to start per CPU socket. \\
\hline
\texttt{I2G\_MPI\_SINGLE\_CORE}        & -pcore
& Set it to 1 to start only one process per core. \\
\hline
\texttt{I2G\_MPI\_PER\_CORE}           & -npcore
& Number of processes to start per core. \\
\hline
\texttt{I2G\_MPI\_NP}                  & -np
& Total number of processes to start.\\
\hline
\end{tabularx} 
\end{center}

These variables can also be set with the \texttt{-d} command line switch. The
following example shows how to set the \texttt{I2G\_MPI\_TYPE} variable to
\texttt{openmpi}:
\begin{verbatim}
mpi-start -d I2G_MPI_TYPE=openmpi
\end{verbatim}

There are also other variables that can modify the behaviour of mpi-start, but
they are described in other sections of this document. The ones dealing with
site configuration of mpi-start are documented in the Site Administrator
Section (\ref{sec:sysadmin}), and the variables dealing with the Hooks are
summarized in Section \ref{sec:hookvars}.

\subsection{Scheduler and Execution Environment Support}

mpi-start support different combinations of batch schedulers and execution
environments using plugins. The schedulers are automatically detected from the
environment and the execution environment can be selected with the
\texttt{I2G\_MPI\_TYPE} variable or the \texttt{-t} command line option. 

\begin{center}
\begin{tabularx}{\textwidth}{lX}
\multicolumn{2}{c} {\textbf{Scheduler Plugins}} \\ \hline
 \texttt{sge} & supports Grid Engine. \\ 
 \texttt{pbs} & for supporting PBS/Torque. \\
 \texttt{lsf} & supports LSF. \\
 \texttt{condor} & gives support for Condor. This plugin lacks the possibility to select how many processes per node should be run. \\
 \texttt{slurm} & for supporting Slurm. As with condor, the plugin currently lacks the processes per node support. \\
 \hline
\end{tabularx} 
\end{center}

\begin{center}
\begin{tabularx}{\textwidth}{lX}
\multicolumn{2}{c} {\textbf{Execution Environment Plugins}} \\ \hline
 \texttt{openmpi} & Open MPI \\ 
 \texttt{mpich2}  & MPICH2\\ 
 \texttt{mpich}   & MPICH \\
 \texttt{lam}     & LAM-MPI \\
 \texttt{pacx}    & PACX-MPI \\
 \texttt{dummy}   & Debugging environment, just executes application in current host. \\ 
 \hline
\end{tabularx} 
\end{center}

\newpage

\section{Hooks}\label{sec:hooks}
The mpi-start Hooks Framework allow the extension of mpi-start features without changing the core functionality.  Several hooks are included in the default distribution of mpi-start for
dealing with file distribution and some MPI extensions. Site admins can check
the local hooks description while users probably are interested in developing
their own hooks for compilation.

\subsection{File Distribution Hooks}

File distribution hooks are responsible for providing a common set of files prior to the execution of the application in all the hosts involved in that execution. Two steps are taken for file distribution:
\begin{itemize}
\item Detection of shared filesystem: mpi-start will try to detect if the
current working directory is in a network file system (currently considered as
such are: nfs, gfs, afs, smb, gpfs and lustre). If the detection is positive,
the distribution of files is not performed. Detection can be totally skipped
by setting: \texttt{MPI\_START\_SHARED\_FS} to 0 or 1 (0 means that mpi-start
will try distribution in the next step, and 1 that it won't). 

\item File distribution: in this step, mpi-start copies files from the current
host to the other hosts involved in the execution. It uses the most suitable
of the available distribution methods. Distribution methods are plugins that
are detected at runtime by checking all the files with \texttt{.filedist}
extension in the mpi-start \texttt{etc} directory.
\end{itemize}

The file distribution method can be fixed by using the \texttt{I2G\_MPI\_FILE\_DIST} variable.

\subsubsection{Distribution Method Plugins}
A file distribution plugin must contain the following functions:

\begin{itemize}
\item \texttt{check\_distribution\_method()}: called during initialization to check if the distribution method is suitable. It returns a number, the lower the number it returns, the higher priority it will have. If the method is not suitable, then it should return 255 or larger.
\item \texttt{copy()} perform the actual copy of files between hosts. Files
are in a gzipped tarball pointed by the \texttt{TARBALL} variable.
\item \texttt{clean()}: clean up any files once the execution is finished.
\end{itemize}

These distribution methods are included in mpi-start:
\begin{description}
\item[ssh] uses scp to copy files, needs passwordless ssh properly configured.
\item[mpiexec] uses OSC mpiexec for copying, needs OSC mpiexec installed.
\item[cptoshared\_area] copies files to a shared area that is not the current working directory. Needs the following variables:
\begin{itemize}
\item \texttt{MPI\_SHARED\_HOME}: set to \texttt{"yes"}.
\item \texttt{MPI\_SHARED\_HOME\_PATH}: path of the shared area that will be used for execution.
\end{itemize}
\item[mpi\_mt] uses mpi\_mt for copying the files. Needs the mpi\_mt binary to
be available in all machines.
\end{description}

\subsection{Extensions Hooks}

Extension hooks are local site hooks that come in the default mpi-start
distribution. The following hooks are available:

\begin{description}
\item [Affinity] The Affinity hook is enabled by setting the
\texttt{MPI\_USE\_AFFINITY} variable to \texttt{1}. When enabled (and the
execution environment supports it), it will define the appropriate options for
setting the processor affinity under the selected MPI implementation.
\item [OpenMP] The OpenMP hook is enabled by setting the \texttt{MPI\_USE\_OMP} variable to \texttt{1}. When enabled it will define the \texttt{OMP\_NUM\_THREADS} environment variable to the number of processors available per mpi process. 
\item [MPItrace] MPItrace is enabled by setting the \texttt{I2G\_USE\_MPITRACE} variable to \texttt{1}. It adds to the execution the mpitrace utility, assuming it is installed at \texttt{MPITRACE\_INSTALLATION}. Once the execution is finished, it gathers and creates the output files at the first host.
\item [MARMOT] Marmot is a tool for analysing and checking MPI programs. This hook enables the use of the tool if the variable \texttt{I2G\_USE\_MARMOT} is set to \texttt{1}. It also copies the analysis output to the first host.
\item [Compiler] This hook sets environment variables \texttt{MPI\_MPI<COMPILER>}, where \texttt{COMPILER} is one of \texttt{CC}, \texttt{F90}, \texttt{F77}, \texttt{CXX}, for C, FORTRAN 90, FORTRAN 77 and C++ compilers respectively. This variables should point to valid compilers for the current MPI implementation. The hook also fixes compiler flags (\texttt{MPI\_MPIxx\_OPTS}) to avoid problems with bad flag for the current processor architecture. This hook can be disabled by setting the environment variable \texttt{MPI\_COMPILER\_HOOK} to \texttt{0}.
\end{description}

These hooks can be completely removed by deleting the affinity.hook, openmp.hook,
mpitrace.hook, marmot.hook, or compiler.hook in the mpi-start configuration
directory.

\subsection{Local Site Hooks}

Site admins can define their own hooks by:
\begin{itemize}
\item Creating new \texttt{.hook} files in the configuration directory, or
\item modifying the \texttt{mpi-start.hooks.local} file.
\end{itemize}

The \texttt{.hook} files are executed in alphabetical order and the \texttt{mpi-start.hooks.local} will be executed after any other hook in the system are executed and the shared file system detection is performed.
Each hook file contains the following functions:
\begin{itemize}
\item \texttt{pre\_run\_hook ()}: it will be executed before the user hooks and the user application gets executed.
\item \texttt{post\_run\_hook ()}: it will be executed after the user application gets executed.
\end{itemize}
If any of these functions is not available, the hook will be ignored.

\subsection{Developing User Hooks}
Users can also customize the mpi-start behavior defining their own hooks by
using the \texttt{-pre} and \texttt{-post} command line switches or by setting the \texttt{I2G\_MPI\_PRE\_RUN\_HOOK}
 and \texttt{I2G\_MPI\_POST\_RUN\_HOOK} environment variables

\begin{description}
\item[\texttt{-pre} / \texttt{I2G\_MPI\_PRE\_RUN\_HOOK}] path of the file
containing the pre-hook, in this file a function called \texttt{pre\_run\_hook()} must be available. This function will be called before the application execution. The pre-hook can be used, for example, to compile the executable itself or download data.

\item[\texttt{-post} / \texttt{I2G\_MPI\_POST\_RUN\_HOOK}] path of the file
containing the post-hook, in this file a function called \texttt{post\_run\_hook()} must be available. This function will be called once the applications finishes its execution. The post-hook can be used to analyze results or to save the results on the grid.
\end{description}

Both pre and post hooks can be in the same file. Next sections contain some hook examples

\subsubsection{Compilation}
Pre-run hook can be used for generating the binaries of the application that will be run by mpi-start. The following sample shows a hook that compiles an application using the C MPI compiler, as defined by the compiler hook in the \texttt{MPI\_MPICC} variable. It assumes that the source code is called like the application binary, but with a \texttt{.c} extension. Use of complex compilation commands like configure, make, etc is also possible. This code is only executed in the first host. The results of the compilation will be available to all hosts thanks to the file distribution mechanisms.

\begin{small}
\begin{verbatim}
#!/bin/sh

# This function will be called before the execution of MPI application
pre_run_hook () {

  # Compile the program.
  echo "Compiling ${I2G_MPI_APPLICATION}"
  $MPI_MPICC $MPI_MPICC_OPTS -o ${I2G_MPI_APPLICATION} ${I2G_MPI_APPLICATION}.c
  if [ ! $? -eq 0 ]; then
    echo "Error compiling program.  Exiting..."
    return 1
  fi
  echo "Successfully compiled ${I2G_MPI_APPLICATION}"
  return 0
}
\end{verbatim}
\end{small}

\subsubsection{Input Preprocessing}
Some applications require some input preprocessing before the application gets executed. For example, gromacs has a \texttt{grompp} tool that prepares the input for the actual \texttt{mdrun} application. In the following example the \texttt{grompp} tool prepares the input for gromacs: 

\begin{small}
\begin{verbatim}
#!/bin/sh

pre_run_hook()
{
   echo "pre_run_hook called"

   # Here comes the pre-mpirun actions of gromacs
   export PATH=$PATH:/$VO_COMPCHEM_SW_DIR/gromacs-3.3/bin
   grompp -v -f full -o full -c after_pr -p speptide -np $MPI_START_NP

   return 0
}
\end{verbatim}
\end{small}

Note the use of the \texttt{MPI\_START\_NP} variable to get the number of processors. See the developer section for a list of internal mpi-start variables. 

\subsubsection{Output Gathering}
Applications that write output files in each of the hosts involved in the
execution may need to fetch all those files to transfer them back to the user
once the execution is finished. The following example copies all the
\texttt{mydata.*} files to the first host. It uses the
\texttt{mpi\_start\_foreach\_host} function of mpi-start that will call the
first argument for each of the hosts passing the name of the host as
parameter. 

\begin{small}
\begin{verbatim}
# the first paramter is the name of a host in the
my_copy () {
    CMD="scp . \$1:\$PWD/mydata.*"
    echo \$CMD
}

post_run_hook () {
    echo "post_run_hook called"
    if [ "x\$MPI_START_SHARED_FS" = "x0" ] ; then
        echo "gather output from remote hosts"
        mpi_start_foreach_host my_copy
    fi
    return 0
}
\end{verbatim}
\end{small}

\subsection{Hooks Variable Summary}\label{sec:hookvars}

This section contains a summary of the variables that can modify the existing
hook behaviour. They can be set using the \texttt{-d} command line switch.

\begin{center}
\begin{tabularx}{\textwidth}{llX}
\textbf{Hook} &  \textbf{Variable}  & \textbf{Meaning}         \\
\hline
File Distribution & \texttt{MPI\_SHARED\_FS} &
If undefined, mpi-start will try to detect a shared file system in the
execution directory. If defined and equal to $1$, mpi-start will assume that the execution directory is shared between all hosts and will not try to copy files. Any other value will make mpi-start assume that the execution directory is not shared. \\ \hline
File Distribution & \texttt{I2G\_MPI\_FILE\_DIST} &
Forces the use of a specific distribution method. \\
\hline
File Distribution & \texttt{MPI\_SHARED\_HOME} &
If set to \texttt{"yes"}, mpi-start will use the path defined in
\texttt{MPI\_SHARED\_HOME\_PATH} for copying the files and executing the
application. \\
\hline
File Distribution & \texttt{MPI\_SHARED\_HOME\_PATH} &
Path to a shared directory. \\
\hline
Affinity & \texttt{MPI\_USE\_AFFINITY} & If set to $1$, enable processor
affinity hook. \\
\hline
OpenMP & \texttt{MPI\_USE\_OMP} & If set to $1$, enable Open MP hook. \\
\hline
MPItrace & \texttt{I2G\_USE\_MPITRACE} & If set to $1$, enable MPItrace hook.\\
\hline
Marmot & \texttt{I2G\_USE\_MARMOT} & If set to $1$, enable Marmot hook.\\
\hline
Compiler & \texttt{MPI\_COMPILER\_HOOK} & If set to $0$, disable compiler hook.\\
\hline
\end{tabularx} 
\end{center}

\newpage

\section{System Administrator Guide}\label{sec:sysadmin}

\subsection{Installation}

\subsubsection{Binary Distribution}

Binary packages for mpi-start are generated in EMI using ETICS. Check their repositories for the correct package for your distribution. Once you have the repositories configured you only need to install the package using your favorite package manager:

For RedHat based distributions:
\begin{small}
\begin{verbatim}
yum install mpi-start
\end{verbatim}
\end{small}

Debian based:
\begin{small}
\begin{verbatim}
apt-get install mpi-start
\end{verbatim}
\end{small}

If you are running a site with CREAM and WN, you may prefer to install the emi-mpi
meta-package that includes the yaim plugin for configuraton:
\begin{small}
\begin{verbatim}
yum install emi-mpi
\end{verbatim}
\end{small}

The nodes where the user applications will be executed (Worker Nodes) also
require a working MPI implementation, Open MPI and MPICH are recommended.
The devel packages should also be installed in order to allow user to compile their
applications. Refer to your OS repositories for the exact packages.
In the case of SL5, Open MPI (including devel packages) can be installed with the following command line:
\begin{small}
\begin{verbatim}
yum install openmpi openmpi-devel
\end{verbatim}
\end{small}

\textbf{devel packages may require also the installation of a C/C++/Fortran
compiler}. Some devel packages of the MPI packages do not include the compiler
as (e.g. gcc, gcc-gfortran, gcc-g++) dependency! They should be installed also
if you want to support the compilation of MPI applications.

\subsubsection{Source Distribution}

Source can be retrieved from the mercurial repository.

Installation is as easy as "make install". The default installation prefix is "/usr". If a non default installation wants to be done, use the PREFIX variable in make install

\begin{small}
\begin{verbatim}
$ make install  PREFIX=/opt/mpi-start
\end{verbatim}
\end{small}

In this case, is recommendable setting the installation the environment
variable I2G\_MPI\_START to point to mpi-start script (although this is not mandatory anymore). 
\begin{small}
\begin{verbatim}
$ export I2G_MPI_START=/opt/mpi-start/bin/mpi-start
\end{verbatim}
\end{small}

\subsection{Configuration}

mpi-start is designed to auto detect most of the site configurations without any administrator intervention. The default installation will automatically detect:
\begin{itemize}
\item the batch scheduler at the site: currently PBS/Torque, SGE, LSF, Condor and Slurm are supported.
\item existence of shared file system in the job running directory
\end{itemize}

mpi-start uses a set of files to configure its behavior. There are several paths where the files can be located. All of them will be checked when looking for hooks, execution environments or scheduler plugins. These are the paths (and the order) used by default in mpi-start:
\begin{itemize}
\item Any path pointed by environment variable \texttt{MPI\_START\_ETC}.
\item A \texttt{.mpi-start} directory at current user's home.
\item The \texttt{etc/mpi-start} under mpi-start installation path. On default installations 
      that would be \texttt{/}.
\end{itemize}

The first file that mpi-start checks is the \texttt{mpi-config.local} file. This should contain the appropriate location of your local MPI installations and any other modifications you want to introduce in the default behavior. mpi-start includes such file with the default configuration for your system in RHEL/SL 5, RHEL/SL 6 and Ubuntu. 

Typical variables that the administrator can set in this file are:

\begin{center}
\begin{tabularx}{\textwidth}{lX}
\textbf{Variable}        &              \textbf{Meaning}         \\ \hline
\texttt{MPI\_DEFAULT\_FLAVOUR} & name of the default flavour for jobs running at the site \\ \hline
\texttt{MPI\_<flavour>\_PATH} & Path of the bin and lib directories for the MPI flavour \\ \hline
\texttt{MPI\_<flavour>\_VERSION} & preferred version of the MPI flavour \\ \hline
\texttt{MPI\_<flavour>\_MPIEXEC} & Path of the MPIEXEC binary for the specific flavour \\ \hline
\texttt{MPI\_<flavour>\_MPIEXEC\_PARAMS} & Parameters for the MPIEXEC of the flavour \\ \hline
\texttt{MPI\_<flavour>\_MPIRUN} & Path of the MPIRUN binary for the specific flavour \\ \hline
\texttt{MPI\_<flavour>\_MPIRUN\_PARAMS} & Parameters for the MPIRUN of the flavour \\ \hline
\texttt{MPI\_<flavour>\_MPI<compiler>} & Location of the compiler for the flavour. Compiler may be one of \texttt{CC}, \texttt{F90}, \texttt{F77} or \texttt{CXX}. \\ \hline
\texttt{I2G\_<flavour>\_PREFIX} & Path of the MPI installation for the MPI flavour \\ \hline
\end{tabularx} 
\end{center}

A known issue with the setting of the I2G\_<flavour>\_PREFIX variable makes
them useless, please use the MPI\_<flavour>\_PATH variable instead!

If \texttt{MPI\_<flavour>\_MPIEXEC} or \texttt{MPI\_<flavour>\_MPIRUN} are not defined, mpi-start will try to use the mpiexec or mpirun that are found in the current PATH.

\subsubsection{Hooks}
Hooks may change the behavior of mpi-start and provide additional features
such as file distribution and configuration of compiler flags. Site admins can
add their own hooks via the local hook mechanism.

mpi-start includes hooks for distributing the files needed for the execution
of an application. By default it tries to find the most suitable method for
copying the files, using shared filesystems whenever they are found. However,
the filesystem detection may not work for all sites, or the shared filesystem
may be in a different location to the execution path making it impossible for
mpi-start to detect its availability. Check Section \ref{sec:hooks} for more
information. Section \ref{sec:hookvars} contains a summary of relevant
variables that may defined.

\subsection{mpi-start Yaim Configuration}

Configuration is necessary on both the CE and WNs in order to support and advertise MPI correctly. This is performed by the yaim MPI module which should be run on both types of nodes.

\subsubsection{WN Configuration}

The yaim plugin in the WN prepares the environment for the correct execution of mpi-start. Each of the MPI flavours supported by the site must be specified setting the variable \texttt{MPI\_<FLAVOUR>\_ENABLE} to \texttt{"yes"}. For example, to enable Open MPI, add the following:

\begin{small}
\begin{verbatim}
MPI_OPENMPI_ENABLE="yes"
\end{verbatim}
\end{small}

Optionally, if you are using a non OS provided MPI implementation, you can
define the location and version with \texttt{MPI\_<FLAVOUR>\_VERSION} and
\texttt{MPI\_<FLAVOUR>\_PATH}. \textbf{Do not use these variables if you are
using the OS provided MPI implementations}. For example for Open MPI version 1.3, installed at /opt/openmpi-1.3:

\begin{small}
\begin{verbatim}
MPI_OPENMPI_VERSION="1.3"
MPI_OPENMPI_PATH="/opt/openmpi-1.3/"
\end{verbatim}
\end{small}

MPI flavours that use a particular mpiexec for starting the jobs (e.g. OSC
mpiexec for PBS/Torque system) may also provide in the
\texttt{MPI\_<FLAVOUR>\_MPIEXEC} the path to the binary. \textbf{Do not use
this variable if you are not using a different mpiexec from the one provided
by the MPI implementation.}

Additionally, you may specify a default MPI flavour to use if non is selected for execution, with the \texttt{MPI\_DEFAULT\_FLAVOUR}. If no default flavour is specified, the first one defined in your site-info.def will be considered as default.

If you provide a shared filesystem for the execution of the applications, but
it is not the path where the jobs are started, then set the variable
\texttt{MPI\_SHARED\_HOME} to \texttt{"yes"} and the variable
\texttt{MPI\_SHARED\_HOME\_PATH} to the the location of the shared filesystem.
\textbf{Do not use this variable if the application starts its execution in a
shared directory (e.g. shared home), this situation should be automatically
detected}.

If you use ssh host based authentication, set the variable \texttt{MPI\_SSH\_HOST\_BASED\_AUTH} to \texttt{"yes"}. \textbf{Note that this does NOT configure passwordless SSH between the nodes, just sets an environmnet variable}

Lastly, if your use a non default location for mpi-start, set its location
with the \texttt{MPI\_MPI\_START} variable.

The complete list of configuration variables for the WN is shown in the next table:
\begin{center}
\begin{tabularx}{\textwidth}{llX}
\textbf{Variable}     & \textbf{Mandatory} & \textbf{Description}        \\ \hline
\texttt{MPI\_<FLAVOUR>\_ENABLE}      & YES & set to \texttt{"yes"} if you want to enable the <flavour> \\ \hline 
\texttt{MPI\_<FLAVOUR>\_VERSION}     & NO  & set to the supported version of the <flavour>, usually is automatically detected \\ \hline
\texttt{MPI\_<FLAVOUR>\_PATH}        & NO  & set to the path of supported version of the <flavour>, usually is automatically detected by the yaim WN plugin \\ \hline
\texttt{MPI\_<FLAVOUR>\_MPIEXEC}     & NO  & If you are using OSC mpiexec (only in PBS/Torque sites), set this to the location of the mpiexec program, e.g. \texttt{"/usr/bin/mpiexec"} \\ \hline
\texttt{MPI\_DEFAULT\_FLAVOUR}       & NO  & Set it to the default flavour for your site, if undefined, the first defined flavour will be used \\ \hline
\texttt{MPI\_SHARED\_HOME}           & NO  & set this to \texttt{"yes"} if you have a shared home area between WNs. \\ \hline
\texttt{MPI\_SHARED\_HOME\_PATH}     & NO  & location of the shared area for execution of MPI applications \\ \hline
\texttt{MPI\_SSH\_HOST\_BASED\_AUTH} & NO  & set it to \texttt{"yes"} if you have SSH based authentication between WNs \\ \hline
\texttt{MPI\_MPI\_START}             & NO  & Location of mpi-start if not installed in standard location (\texttt{/usr/bin/mpi-start}) \\ \hline
\end{tabularx}
\end{center}

The profile for a worker node is MPI\_WN. Use it along any other profiles you may need for your WN. 
\begin{small}
\begin{verbatim}
/opt/glite/yaim/bin/yaim -c -s site-info.def  -n MPI_WN -n <other_WN_profiles>
\end{verbatim}
\end{small}

\subsubsection{CE Configuration}

As with the WN, individual flavours of MPI are enabled by setting the \texttt{MPI\_<FLAVOUR>\_ENABLE} 
associated variable to \texttt{"yes"}. The version of the MPI implementation must also
be specified with the variable \texttt{MPI\_<FLAVOUR>\_VERSION}, e.g. for configuring Open MPI version 1.3:

\begin{small}
\begin{verbatim}
MPI_OPENMPI_ENABLE="yes"
MPI_OPENMPI_VERSION="1.3"
\end{verbatim}
\end{small}

Possible flavours are:
\begin{description}
\item[OPENMPI] for Open MPI
\item[MPICH] for MPICH-1
\item[MPICH2] for MPICH-2
\item[LAM] for LAM-MPI
\end{description}

The use of shared homes should be announced also by setting the \texttt{MPI\_SHARED\_HOME} to \texttt{"yes"}.

If you are using PBS/Torque, you can set the variable
\texttt{MPI\_SUBMIT\_FILTER} to \texttt{"yes"} in order to enable the
submission of parallel jobs in your system. The submit filter assumes that
your Worker Nodes are correctly configured to publish in their status the \texttt{ncpus}
variable with the number of available slots. If that's not true in your case,
you may edit the file \texttt{/var/torque/submit\_filter} in line 71 to fit
your pbsnodes output. An example for using the \texttt{np} value is commented
out in the file.

The complete list of configuration variables for the CE is shown in the next table:
\begin{center}
\begin{tabularx}{\textwidth}{llX}
\textbf{Variable}     & \textbf{Mandatory} & \textbf{Description}        \\ \hline
\texttt{MPI\_<FLAVOUR>\_ENABLE}    & YES & set to \texttt{"yes"} if you want to enable the <flavour> \\ \hline
\texttt{MPI\_<FLAVOUR>\_VERSION}   & YES & set to the supported version of the <flavour>, usually is automatically detected \\ \hline
\texttt{MPI\_START\_VERSION}       & NO  & set to the available mpi-start version. If not set, the yaim plugin will try to figure out the version by checking if mpi-start is installed. \\ \hline
\texttt{MPI\_SHARED\_HOME}         & NO  & set this to \texttt{"yes"} if you have a shared home area between WNs. \\ \hline
\texttt{MPI\_SUBMIT\_FILTER}       & NO  & Set this to \texttt{"yes"} to configure the submit filter for torque batch system that enables the submission of parallel jobs. The configuration assumes that torque path is \texttt{/var/torque} or \texttt{TORQUE\_VAR\_DIR} variable if defined. \\ \hline
\end{tabularx}
\end{center}

The profile for configuring the CE is MPI\_CE. 
\begin{small}
\begin{verbatim}
/opt/glite/yaim/bin/yaim -c -s site-info.def  -n MPI_CE -n <other_ce_profiles>
\end{verbatim}
\end{small}

\textbf{Batch system and MPI}: The batch system may need extra configuration for the submission of MPI jobs.
In PBS, you may use the automatic creation of the submit filter with the
\texttt{MPI\_SUBMIT\_FILTER} variable. \textbf{Note:} any changes to the submit filter will be overwritten if yaim is re-run.
In the case of SGE you need to configure a parallel environment.
Check the documentation of your batch system for any further details.

For glite-yaim-mpi versions <= 1.1.11, the submit filter assumes that the pbsnodes -a output has the \texttt{ncpus=} field in the status line correctly set. If not, please change the submit filter like shown in this diff:

\begin{small}
\begin{verbatim}
--- submit_filter       2012-01-20 11:19:48.000000000 +0100
+++ submit_filter.new   2012-01-20 11:19:21.000000000 +0100
@@ -68,8 +68,8 @@
         if (m/^\s*state\s*=\s*(\w+)/) {
             $state = ($1 eq "offline") ? 0 : 1;
         # This may be changed to fit your nodes description
-        # } elsif (m/^\s*np\s*=\s*(\d+)/) {
-        } elsif (m/^\s*status\s*=\s*.*ncpus=(\d+),/) {
+        } elsif (m/^\s*np\s*=\s*(\d+)/) {
+        # } elsif (m/^\s*status\s*=\s*.*ncpus=(\d+),/) {
             my $ncpus = $1;
             if ($state) {
                 if (defined($machines{$ncpus})) {
\end{verbatim}
\end{small}

The default behaviour of the submit filter has changed in version 1.1.11 to use the "np=xx" parameter of the pbsnodes command output. Check the patch shown previously for the changes applied. 

\textbf{MPI\_CE and other yaim profiles}: The \texttt{MPI\_CE} profile should
be the first in the yaim configuration, otherwise the Glue variables will not
be properly defined. This restriction may be removed in future versions.

\textbf{mpi-start version}: The yaim plugin will publish in the tags the mpi-start version if mpi-start is installed at the CE. If not installed you should define the \texttt{MPI\_START\_VERSION} with the version available at the WNs.

\subsubsection{Example configuration}

Here is an example configuration (with both CEs and WN variables!):

\begin{small}
\begin{verbatim}
#----------------------------------
# MPI-related configuration:
#----------------------------------
# Several MPI implementations (or "flavours") are available.
# If you do NOT want a flavour to be installed/configured, set its variable
# to "no". Else, set it to "yes" (default). If you want to use an
# already installed version of an implementation, set its "_PATH" and
# "_VERSION" variables to match your setup (examples below).
#
# NOTE 1: the CE_RUNTIMEENV will be automatically updated in the file
# functions/config_mpi, so that the CE advertises the MPI implementations
# you choose here - you do NOT have to change it manually in this file.
# It will become something like this:
#
#   CE_RUNTIMEENV="$CE_RUNTIMEENV
#              MPI_MPICH
#              MPI_MPICH2
#              MPI_OPENMPI
#              MPI_LAM"
#
# NOTE 2: it is currently NOT possible to configure multiple concurrent
# versions of the same implementations (e.g. MPICH-1.2.3 and MPICH-1.2.7)
# using YAIM. Customize "/opt/glite/yaim/functions/config_mpi" file
# to do so.

MPI_MPICH_ENABLE="yes"
MPI_MPICH_VERSION="1.2.7p1"

MPI_MPICH2_ENABLE="yes"
MPI_MPICH2_VERSION="1.0.4"

MPI_OPENMPI_ENABLE="yes"
MPI_OPENMPI_VERSION="1.1"

MPI_LAM_ENABLE="yes"
MPI_LAM_VERSION="7.1.2"

# set Open MPI as default flavour
MPI_DEFAULT_FLAVOUR=OPENMPI

#---
# Example for using an already installed version of MPI.
# Setting "_PATH" and "_VERSION" variables will prevent YAIM
# from using the default OS packages.
# Just fill in the path to its current installation (e.g. "/usr")
# and which version it is (e.g. "6.5.9").
# DO NOT USE UNLESS A NON DEFAULT LOCATION IS USED
#---
# MPI_MPICH_PATH="/opt/mpich-1.2.7p1/"
# MPI_MPICH2_PATH="/opt/mpich2-1.0.4/"

# If you do NOT provide a shared home, set $MPI_SHARED_HOME to "no" (default).
#
# MPI_SHARED_HOME="yes"

#
# If you do NOT have SSH Hostbased Authentication between your WNs, set the below
# variable to "no" (default). Else, set it to "yes".
#
# MPI_SSH_HOST_BASED_AUTH="yes"


# If you use Torque as batch system, you may want to let the yaim plugin
# configure a submit filter for you. Uncomment the following line to do so
# MPI_SUBMIT_FILTER="yes"

#
# If you provide an 'mpiexec' for MPICH or MPICH2, please state the full path to
# that file here (http://www.osc.edu/~pw/mpiexec/index.php). Else, leave empty.
#
#MPI_MPICH_MPIEXEC="/usr/bin/mpiexec"
\end{verbatim}
\end{small}

\newpage

\section{Examples}

\subsection{Simple Job}

Simple job using environment variables:

\begin{small}
\begin{verbatim}
#!/bin/sh
# IMPORTANT : This example script execute a
#             non-mpi program with Open MPI
#
export I2G_MPI_APPLICATION=/bin/hostname
export I2G_MPI_TYPE=openmpi

$I2G_MPI_START
\end{verbatim}
\end{small}

Same example using command line parameters:
\begin{small}
\begin{verbatim}
mpi-start -t openmpi /bin/hostname
\end{verbatim}
\end{small}

\subsection{Job with User Hooks}
\begin{small}
\begin{verbatim}
#!/bin/sh
#
# MPI_START_SHARED_FS can be used to figure out if the current working
# is located on a shared file system or not. (1=yes, 0=no);
#
# The "mpi_start_foreach_host" function takes as parameter the name of
# another function that will be called for each host in the machine as
# first parameter.
# - For each host the callback function will be called exactly once,
#   independent how often the host appears in the machinefile.
# - The callback function will also be called for the local host.

# create the pre-run hook
cat > pre_run_hook.sh << EOF

pre_run_hook () {
    echo "pre run hook called "
    # - download data
    # - compile program

    if [ "x\$MPI_START_SHARED_FS" = "x0" ] ; then
        echo "If we need a shared file system we can return -1 to abort"
        # return -1
    fi

    return 0
}
EOF

# create the post-run hook
cat > post_run_hook.sh << EOF
# the first paramter is the name of a host in the
my_copy () {
    CMD="scp . \$1:\$PWD/mydata.1"
    echo \$CMD
    #\$CMD
    # upload data
}

post_run_hook () {
    echo "post_run_hook called"
    if [ "x\$MPI_START_SHARED_FS" = "x0" ] ; then
        echo "gather output from remote hosts"
        mpi_start_foreach_host my_copy
    fi
    return 0
}
EOF

export I2G_MPI_APPLICATION=mpi_sleep
export I2G_MPI_APPLICATION_ARGS=0
export I2G_MPI_TYPE=openmpi
export I2G_MPI_PRE_RUN_HOOK=./pre_run_hook.sh
export I2G_MPI_POST_RUN_HOOK=./post_run_hook.sh

$I2G_MPI_START

# instead of the variable definition, the following command line could be used:
# mpi-start -t openmpi -pre ./pre_run_hook.sh -post ./post_run_hook.sh mpi_sleep 0
\end{verbatim}
\end{small}

\subsection{Using mpi-start with WMS}

EMI provides the WMS software for submitting jobs to the different available resources. The WMS gets a job description in the JDL language and performs the selection and actual submission of the job into the resources on behalf of the user. The following sections describe how to submit a job using the WMS.

\subsubsection{Basic Job Submission}
Jobs are described with the JDL language. Most relevant attributes for parallel job submission are:
\begin{itemize}
\item CPUNumber: number of processes to allocate.
\item Requirements: requirements of the job, will allow to force the selection of sites with mpi-start support.
\end{itemize}

The following example shows a job that will use 6 processes and it is executed with Open MPI. The \texttt{requirements} attribute makes the WMS to select sites that publish that they support mpi-start and Open MPI.

\begin{small}
\begin{verbatim}
JobType       = "Normal";
CPUNumber     = 6;
Executable    = "starter.sh";
Arguments     = "OPENMPI hello_bin hello arguments";
InputSandbox  = {"starter.sh", "hello_bin"};
OutputSandbox = {"std.out", "std.err"};
StdOutput     = "std.out";
StdError      = "std.err";
Requirements  = member("MPI-START", other.GlueHostApplicationSoftwareRunTimeEnvironment)
                && member("OPENMPI", other.GlueHostApplicationSoftwareRunTimeEnvironment);
\end{verbatim}
\end{small}

The \texttt{Executable} attribute is a script that will invoke mpi-start with the correct options for the execution of the user's application. We propose a generic wrapper that can be used for any application and MPI flavour that gets in the \texttt{Arguments} attribute:

\begin{itemize}
\item Name of mpi-start execution environment (I2G\_MPI\_FLAVOUR variable), in the example: OPENMPI
\item Name of user binary, in the example: hello\_bin
\item Arguments for the user binary, in the example: hello arguments
\end{itemize}

This is the content of the wrapper:

\begin{small}
\begin{verbatim}
#!/bin/bash
# Pull in the arguments.
MPI_FLAVOR=$1

MPI_FLAVOR_LOWER=`echo $MPI_FLAVOR | tr '[:upper:]' '[:lower:]'`
export I2G_MPI_TYPE=$MPI_FLAVOR_LOWER

shift
export I2G_MPI_APPLICATION=$1

shift
export I2G_MPI_APPLICATION_ARGS=$*

# Touch the executable, and make sure it's executable.
touch $I2G_MPI_APPLICATION
chmod +x $I2G_MPI_APPLICATION

# Invoke mpi-start.
$I2G_MPI_START
\end{verbatim}
\end{small}

User needs to include this wrapper in the \texttt{InputSandbox} of the JDL (\texttt{starter.sh}) and set it as the \texttt{Executable} of the job. Submission is performed as any other job:

\begin{small}
\begin{verbatim}
$ glite-wms-job-submit -a hello-mpi.sh

Connecting to the service https://gridwms01.ifca.es:7443/glite_wms_wmproxy_server


====================== glite-wms-job-submit Success ======================

The job has been successfully submitted to the WMProxy
Your job identifier is:

https://gridwms01.ifca.es:9000/8jG3MUNRm-ol7BqhFP5Crg

==========================================================================
\end{verbatim}
\end{small}

Once the job is finished, the output can be retrieved:

\begin{small}
\begin{verbatim}
$ glite-wms-job-output https://gridwms01.ifca.es:9000/8jG3MUNRm-ol7BqhFP5Crg

Connecting to the service https://gridwms01.ifca.es:7443/glite_wms_wmproxy_server

================================================================================

                        JOB GET OUTPUT OUTCOME

Output sandbox files for the job:
https://gridwms01.ifca.es:9000/8jG3MUNRm-ol7BqhFP5Crg
have been successfully retrieved and stored in the directory:
/gpfs/csic_projects/grid/tmp/jobOutput/enol_8jG3MUNRm-ol7BqhFP5Crg

================================================================================


$ cat /gpfs/csic_projects/grid/tmp/jobOutput/enol_8jG3MUNRm-ol7BqhFP5Crg/std.*
Hello world from gcsic054wn. Process 3 of 6
Hello world from gcsic054wn. Process 1 of 6
Hello world from gcsic054wn. Process 2 of 6
Hello world from gcsic054wn. Process 0 of 6
Hello world from gcsic055wn. Process 4 of 6
Hello world from gcsic055wn. Process 5 of 6
\end{verbatim}
\end{small}

\subsubsection{Modifying mpi-start Behavior}
mpi-start behavior can be customized by setting different environment
variables. If using the generic wrapper, one easy way of customizing mpi-start
execution is using the \texttt{Environment} attribute of the JDL. The following JDL adds debugging to
the previous example by setting the \texttt{I2G\_MPI\_START\_VERBOSE} and
\texttt{I2G\_MPI\_START\_DEBUG} variables to 1:

\begin{small}
\begin{verbatim}
JobType       = "Normal";
CPUNumber     = 6;
Executable    = "starter.sh";
Arguments     = "OPENMPI hello_bin hello arguments";
InputSandbox  = {"starter.sh", "hello_bin"};
OutputSandbox = {"std.out", "std.err"};
StdOutput     = "std.out";
StdError      = "std.err";
Requirements  = member("MPI-START", other.GlueHostApplicationSoftwareRunTimeEnvironment)
             && member("OPENMPI", other.GlueHostApplicationSoftwareRunTimeEnvironment);
Environment   = {"I2G_MPI_START_VERBOSE=1", "I2G_MPI_START_DEBUG=1"};
\end{verbatim}
\end{small}

Use of hooks is also possible using this mechanism. If the user has a file
with the mpi-start hooks called \texttt{hooks.sh}, the following JDL would add
it to the execution (notice that the file is also added in the
\texttt{InputSandbox}):

\begin{small}
\begin{verbatim}
JobType       = "Normal";
CPUNumber     = 6;
Executable    = "starter.sh";
Arguments     = "OPENMPI hello_bin hello arguments";
InputSandbox  = {"starter.sh", "hello_bin", "hooks.sh"};
OutputSandbox = {"std.out", "std.err"};
StdOutput     = "std.out";
StdError      = "std.err";
Requirements  = member("MPI-START", other.GlueHostApplicationSoftwareRunTimeEnvironment)
             && member("OPENMPI", other.GlueHostApplicationSoftwareRunTimeEnvironment);
Environment   = {"I2G_MPI_PRE_RUN_HOOK=hooks.sh", "I2G_MPI_POST_RUN_HOOK=hooks.sh"};
\end{verbatim}
\end{small}

%\newpage
%\section{Known Issues}
%This sections contains any known issues in the software and possible solutions.
%\begin{itemize}
%\item Fine grained process mapping is not supported with Slurm or Condor schedulers.
%\end{itemize}

\newpage
\section{mpi-start Internals}

This section documents the internal variables of mpi-start. They might be used for development of hooks or configuration of mpi-start. As the documentation improves, these may be moved to other sections of the document.

\subsection{Global Configuration Variables}
\begin{center}
\begin{tabularx}{\textwidth}{llX}
\textbf{Variable}     & \textbf{Default} & \textbf{Description}        \\ \hline
\texttt{MPI\_START\_DUMMY\_SCHEDULER} & 1 & Enables or disables the dummy scheduler. \\ \hline
\texttt{I2G\_MPI\_START\_KEEP\_FILES} & 0 & 
       Enables or disables the removal of temporary files at the end of execution.\\ \hline
\texttt{I2G\_MPI\_START\_FULL\_TRACE} & - & 
       Enables or disables full trace of mpi-start. \\ \hline
\texttt{MPI\_START\_DO\_NOT\_USE\_WRAPPER} & - &
       Enables or disables the use of a wrapper for the executable \\ \hline
\texttt{MPI\_START\_SOCKETS} & - & 
       Number of sockets in the host (if not defined, mpi-start tries to detect them). \\ \hline
\texttt{MPI\_START\_COREPERSOCKET} & - & 
       Number of cores per socket in the host (if not defined, mpi-start tries to detect them). \\ \hline
\texttt{MPI\_START\_COREPERSOCKET} & - & 
       Number of cores per socket in the host (if not defined, mpi-start tries to detect them). \\ \hline
\texttt{I2G\_MPI\_START\_ENABLE\_TESTING} & - &
      If equal to \texttt{"TEST"}, then do not call the main function. Used for sourcing the mpi-start file.  \\ \hline
\end{tabularx}
\end{center}

\subsection{Scheduler plugin variables}
\begin{center}
\begin{tabularx}{\textwidth}{lX}
\textbf{Variable}     & \textbf{Description}        \\ \hline
\texttt{MPI\_START\_SCHEDULER} & Name of the scheduler. \\ \hline
\texttt{MPI\_START\_HOSTFILE} & File containing one line per slot available. \\ \hline
\texttt{MPI\_START\_MACHINEFILE} & File containing one line per host available. \\ \hline
\texttt{MPI\_START\_HOST\_SLOTS\_FILE} & File containing one line with a name of host, and the number of slots available in that host. \\ \hline
\texttt{MPI\_START\_NP} & Total number of processors to use. \\ \hline
\texttt{MPI\_START\_NPHOST} & Number of processes per host, may be undefined if slot allocation is used. \\  \hline.
\end{tabularx}
\end{center}

\subsection{MPI Execution Variables}
\begin{center}
\begin{tabularx}{\textwidth}{lX}
\textbf{Variable}            & \textbf{Description}        \\ \hline
\texttt{MPIEXEC}             & Defined by each flavour, mpiexec executable \\ \hline
\texttt{MPI\_GLOBAL\_PARAMS} & Global parameters for mpiexec \\ \hline
\texttt{MPI\_LOCAL\_PARAMS}  & Local parameters for mpiexec \\ \hline
\texttt{MPI\_START\_SSH\_AGENT} & User (or system) specified ssh agent (used mostly by condor). \\ \hline
\texttt{MPI\_START\_DISABLE\_LRMS\_INTEGRATION} & If set to \texttt{"yes"}, do not use any LRMS integration available in the MPI flavour. \\ \hline
\texttt{MPI\_MPICH2\_DISABLE\_HYDRA} & If set to \texttt{1}, disable the use of hydra launcher for mpich2. \\  \hline
\texttt{OSC\_MPIEXEC} & Set to \texttt{1} if OSC mpiexec is found. \\ \hline
\texttt{HYDRA\_MPIEXEC} & Set to \texttt{1} if hydra mpiexec is found. \\ \hline
\texttt{OPENMPI\_VERSION\_MAJOR} & Set to Open MPI major version. \\ \hline
\texttt{OPENMPI\_VERSION\_MINOR} & Set to Open MPI minor version. \\ \hline
\texttt{OPENMPI\_VERSION\_RELEASE} & Set to Open MPI release version. \\ \hline
\end{tabularx}
\end{center}

\newpage
\appendix
\section{Configuration of batch system}

The batch system must be ready to execute parallel jobs (i.e. more than one slot is requested for a single job).
Each batch system has its own specific ways of configuring such support. 

Here you can find the instructions to manually configure different batch systems to execute MPI jobs. 

\subsection{Torque/PBS}

Torque/PBS can be configured with the yaim module as described in previous sections. In order to configure manually you will need to edit (create it if it does not exist) your torque configuration file (\texttt{/var/torque/torque.cfg} or \texttt{/var/spool/pbs/torque.cfg}) and add a line containing:

\begin{verbatim}
SUBMITFILTER /var/torque/submit_filter.pl
\end{verbatim}

Then download the \texttt{submit\_filter.pl} from \url
{http://devel.ifca.es/rep/submit\_filter.pl} and put it in the above location.

This filter modifies the script coming from the submission, rewriting the \texttt{-l nodes=XX} option with specific requests, based on the information given by \texttt{pbsnodes} command.

The submit filter is crucial. Failing to use the submit filter translates in the job being submitted to only one node, where all the MPI processes are allocated too, instead of distributing the job across several nodes.

\textbf{Warning: updates tend to rewrite torque.cfg. Check that the submit filter line is still there after performing an update}

\subsubsection{Maui}

Edit your configuration file (usually under \texttt{/var/spool/maui/maui.cfg}) and check that it contains the following line:

\begin{verbatim}
ENABLEMULTIREQJOBS TRUE
\end{verbatim}

The \texttt{ENABLEMULTINODEJOBS} parameter must not be set to FALSE (if not specified is TRUE by default). These parameters allow a job to span to more than one node and to specify multiple independent resource requests.

The maui version provided as third party in EMI/UMD (maui-3.2.6p21-snap.1234905291.5.el5) has a bug that prevents the use of more than one WN when submitting a parallel job. See \url{https://ggus.eu/ws/ticket\_info.php?ticket=57828} for details. It is recommended to use newer versions of maui that do not have this problem.


%=== Testing your configuration ===
%
%To be written
%

\subsection{SGE}

Support for parallel jobs under SGE is enabled using \emph{Parallel Environments}. You will need to configure at least one parallel environment in order to execute the jobs. Check the Parallel Environment documentation for more information. The CREAM Blah scripts will automatically use the available parallel environment if a job that requires more than one CPU is submitted.

In the following example a PE configuration is shown:
\begin{verbatim}
[root@ce ~]# qconf -sp mpi
pe_name            mpi
slots              4    
user_lists         NONE
xuser_lists        NONE
start_proc_args    /bin/true
stop_proc_args     /bin/true
allocation_rule    $fill_up
control_slaves     TRUE   
job_is_first_task  FALSE
\end{verbatim}

%=== Testing your configuration ===
%
%''To be written''
%
%== Other batch systems ==
%
%''To be written''
%

\subsection{Passwordless ssh (hostbased authentication)}

Depending on the MPI implementation used and if the site does not have a shared file system, passwordless ssh between the nodes may be required between the WN. If that's the case, make sure that any pool account can login from one WN to any other using ssh without showing any password prompt.

\section{Installation of MPI implementation}

In order to execute MPI jobs, the site must support one of the multiple MPI implementations available. Most extended are Open MPI and MPICH2. OS distributions provide ready to use packages that fit most use cases. SL provides the following packages:
\begin{itemize}
\item \texttt{openmpi} and \texttt{openmpi-devel} for Open MPI.
\item \texttt{mpich2} and \texttt{mpich2-devel} for MPICH2.
\item \texttt{lam} and \texttt{lam-devel} for LAM
\end{itemize}

Installation of devel packages for the MPI implementation is recommended, since this will allow users to compile their applications at the site. Moreover the Nagios probes will try to compile a binary, thus not having a working compiler will make them fail. Note that the compiler may not be specified as dependencies of the \texttt{-devel} packages. Make sure that \texttt{gcc} and related packages are available.

Note also that not all the implementations support tight integration with the batch system. \textbf{Tight integration is required for proper accounting numbers.}

The MPI packages must be installed at the nodes that will execute the jobs (WN).

\subsection{Open MPI}

Open MPI support tight integration with several batch system, including Torque/PBS and SGE, that may require recompilation of the packages in order to get it. Tight integration allows proper accounting of resources used (CPU time) and better control of the jobs by the system, thus avoiding zombie processes if something goes wrong with the application. The following sections describe the SGE and PBS/Torque cases:

\subsubsection{SGE}

The SGE tight scheduler integration allows Open MPI to start the processes in the worker nodes using the native batch system utilities, thus providing better process control and accounting. SL5 packages already include support for SGE with the \texttt{openmpi} and \texttt{openmpi-devel} rpms. After Open MPI is installed, you should see one component named gridengine in the ompi\_info output:

\begin{verbatim}
$ ompi_info | grep gridengine
                MCA ras: gridengine (MCA v2.0, API v2.0, Component v1.4)
\end{verbatim}

Check the Open MPI FAQ at \url{http://www.open-mpi.org/faq/?category=building#build-rte-sge} for more information.

\subsubsection{Torque/PBS}

In the case of Torque/PBS in SL5 you will need to compile the packages for your site. The Open MPI FAQ (\url{http://www.open-mpi.org/faq/?category=building#build-rte-tm}) includes instructions for doing so. You can adapt the SL5 packages to support Torque/PBS following these steps:

\begin{itemize}
\item Download and install Open MPI source rpm: \url{http://ftp2.scientificlinux.org/linux/scientific/5x/SRPMS/vendor/openmpi-1.4-4.el5.src.rpm}
\begin{small}
\begin{verbatim}
$ rpm -Uvh http://ftp2.scientificlinux.org/linux/scientific/5x/SRPMS/vendor/openmpi-1.4-4.el5.src.rpm
Retrieving http://ftp2.scientificlinux.org/linux/scientific/5x/SRPMS/vendor/openmpi-1.4-4.el5.src.rpm
warning: /var/tmp/rpm-xfer.DAMscP: Header V3 DSA signature: NOKEY, key ID 192a7d7d
   1:openmpi                warning: user mockbuild does not exist - using root
warning: group mockbuild does not exist - using root
########################################### [100%]
warning: user mockbuild does not exist - using root
warning: group mockbuild does not exist - using root
warning: user mockbuild does not exist - using root
warning: group mockbuild does not exist - using root
warning: user mockbuild does not exist - using root
warning: group mockbuild does not exist - using root
warning: user mockbuild does not exist - using root
warning: group mockbuild does not exist - using root
warning: user mockbuild does not exist - using root
warning: group mockbuild does not exist - using root
warning: user mockbuild does not exist - using root
warning: group mockbuild does not exist - using root
\end{verbatim}
\end{small}


\item Modify the spec file to include Torque/PBS support:
\begin{small}
\begin{verbatim}
--- openmpi.spec        2010-03-31 23:18:20.000000000 +0200
+++ openmpi.spec        2011-03-07 18:37:11.000000000 +0100
@@ -114,6 +114,7 @@
 ./configure --prefix=%{_libdir}/%{mpidir} --with-libnuma=/usr \
        --with-openib=/usr --enable-mpirun-prefix-by-default \
        --mandir=%{_libdir}/%{mpidir}/man %{?with_valgrind} \
+        --with-tm \
        --enable-openib-ibcm --with-sge \
        CC=%{opt_cc} CXX=%{opt_cxx} \
        LDFLAGS='-Wl,-z,noexecstack' \
\end{verbatim}
\end{small}

\item Install Torque/PBS development libraries:
\begin{small}
\begin{verbatim}
$ yum install libtorque-devel
\end{verbatim}
\end{small}

\item Build the RPMs
\begin{small}
\begin{verbatim}
$ rpmbuild -ba /usr/src/redhat/SPECS/openmpi.spec
\end{verbatim}
\end{small}

\item Install the resulting RPMs:
\begin{small}
\begin{verbatim}
$ yum localinstall -nogpgcheck /usr/src/redhat/RPMS/x86_64/openmpi-*
\end{verbatim}
\end{small}

\item Check that the support for Torque/PBS is enabled:
\begin{small}
\begin{verbatim}
$ /usr/lib64/openmpi/1.4-gcc/bin/ompi_info | grep tm
              MCA memory: ptmalloc2 (MCA v2.0, API v2.0, Component v1.4)
                 MCA ras: tm (MCA v2.0, API v2.0, Component v1.4)
                 MCA plm: tm (MCA v2.0, API v2.0, Component v1.4)
\end{verbatim}
\end{small}

\end{itemize}

\subsubsection{Open MPI without tight integration}

Open MPI can use rsh/ssh for starting the jobs if no tight integration is available. Jobs will run if you have passwordless ssh enabled between the WN, but the accounting figures will be incorrect.

\subsection{MPICH2}

MPICH2 can use several launchers for starting the processes:
\begin{itemize}
\item MPD, which uses rsh/ssh for starting the processes, so it will not produce correct accounting numbers. 
\item Hydra, which also uses rsh/ssh and should support tight integration with some batch systems.
\item For PBS/Torque, OSC Mpiexec \url{http://www.osc.edu/~djohnson/mpiexec/index.php} which includes tight integration.
\end{itemize}

mpi-start is able to select the most appropriate one if found (Hydra is prefered over MPD)

\subsubsection{MPD}

MPD is available in all versions of MPICH2 and uses rsh/ssh to start the processes. It was the default starter for versions < 1.3. It uses a .mpd.conf file at the home directory, so it is necessary to provide a way to access the home directory from the WN (usual case)

\subsubsection{Hydra}

Hydra is the new starter of MPICH2 and the default since version 1.3. It is designed to natively work with multiple daemons such as ssh, rsh, pbs, slurm and sge. Notice that not all versions support all the daemons!. The version included with SL5 \textbf{does NOT support pbs or sge}, therefore passwordless rsh/ssh between the nodes is mandatory.

sge support is included since version 1.3b1. pbs is included in version 1.5a1. If you want to have tight integration (i.e. accounting) with MPICH2 and one of these systems you may need to download and compile the packages at your site.

\subsubsection{OSC Mpiexec}

OSC Mpiexec provides tight integration for PBS/Torque system. In order to use itwith mpi-start you will need to define the variable \texttt{MPI\_MPICH2\_MPIEXEC} pointing to its location.
%
%''OSC mpiexec installation: to be written''
%

\section{Distribution of binaries}

The MPI binaries that users want to run need to be accessible on every node involved in an MPI computation (it is a parallel job after all). There are three main approaches:

\subsection{Shared home/other shared area}

By far the best option is to provide user homes hosted on a shared filesystem. This could either be a network filesystem (e.g. NFS) or a cluster filesystem (e.g. GPFS or Lustre). Then the MPI binary you transfer in the Sandbox (or compile up) on the first MPI node will automatically be available on all nodes. This is the normal mode of operation for MPI, and what MPI users will probably expect.

mpi-start checks if the working directory of the job is in a shared filesystem (nfs, gfs, afs, smb, gpfs and lustre are detected) and considers that if the filesystem is shared the binaries will be available without any further action in all the nodes involved in the execution. 

In some cases, there is an available shared filesystem but the job does not start its execution there. Site admins can force mpi-start to use one directory as shared for transferring the job files to all nodes as described in the hooks section.

\subsubsection{Passwordless ssh between WNs}

If you configure host-based authentication between worker nodes, then mpi-start can automatically replicate your binary to nodes involved in the computation. All the files in the working directory will be replicated, however, other needed files (e.g. data) may not be replicated, so this would have to be done manually (and would be slow for large data sets). Also it could open up the potential for users to subvert the normal resource management mechanisms by directly executing commands on nodes not allocated to them.

\subsubsection{Use OSC mpiexec to distribute files}

This option is for sites with neither a shared filesystem nor passwordless ssh between WNs. If you have an mpiexec that can spawn the remote jobs using the LRMS native interface, you can use it to distribute the files. See \url{http://www.osc.edu/~djohnson/mpiexec/index.php#Cute\_mpiexec\_hacks} for the basic idea.

\section{Information System}

Sites may install different implementations (or flavours) of MPI. It is important therefore that users can use the information system to locate sites with the software they require. You should publish some values to let the world know which flavour of MPI you are supporting, as well as the interconnect and some other things. Everything related with MPI should be published as GlueHostApplicationSoftwareRunTimeEnvironment in the corresponding sections.

\subsection{MPI-start support}

May include the version.

\begin{verbatim}
GlueHostApplicationSoftwareRunTimeEnvironment: MPI-START
GlueHostApplicationSoftwareRunTimeEnvironment: MPI-START-1.3.0
\end{verbatim}

\subsection{MPI flavour(s)}

<MPI flavour>

This is the most basic variable and one should be advertised for each MPI flavour that has been installed and tested. Currently supported flavours are MPICH, MPICH2, LAM and OPENMPI.

Example:
\begin{verbatim}
GlueHostApplicationSoftwareRunTimeEnvironment: MPICH
GlueHostApplicationSoftwareRunTimeEnvironment: MPICH2
GlueHostApplicationSoftwareRunTimeEnvironment: LAM
GlueHostApplicationSoftwareRunTimeEnvironment: OPENMPI
\end{verbatim}

\subsection{MPI version(s)}

<MPI flavour>-<MPI version>

This should be published to allow users with special requirements to locate specific versions of MPI software.

Examples:
\begin{verbatim}
GlueHostApplicationSoftwareRunTimeEnvironment: OPENMPI-1.0.2
GlueHostApplicationSoftwareRunTimeEnvironment: MPICH-1.2.7
GlueHostApplicationSoftwareRunTimeEnvironment: MPICH-G2-1.2.7
GlueHostApplicationSoftwareRunTimeEnvironment: OPENMPI-1.0.2-ICC
\end{verbatim}

\subsection{MPI compiler(s) -- optional}

<MPI flavour>-<MPI version>-<Compiler>

If <Compiler> is not published, then gcc suite is assumed.

\subsection{Interconnects -- optional}

MPI-<interconnect>

Interconnects: Ethernet, Infiniband, SCI, Myrinet

Example:
\begin{verbatim}
GlueHostApplicationSoftwareRunTimeEnvironment: MPI-Infiniband
\end{verbatim}

\subsection{Shared homes}

If a site has a shared filesystem for home directories it should publish the variable \texttt{MPI\_SHARED\_HOME}.

\begin{verbatim}
GlueHostApplicationSoftwareRunTimeEnvironment: MPI_SHARED_HOME
\end{verbatim}


\end{document}
