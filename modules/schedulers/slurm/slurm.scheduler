#!/bin/bash

#
# Copyright (c) 2010 Instituto de Fisica de Cantabria. CSIC.
#                    All rights reserved.
#


#
# XXX
# SLURM Support lacks the fine grained mapping options of the other
# scheduler.
#

SCHEDULER_NAME="slurm"

#
# This function checks if the current job is running in a slurm
# environment.
#
# Return values :
#  0     - Support for this kind of scheduler is found.
#  else  - NO support for this of scheduler is found. 
#
scheduler_available () {
    debug_msg " checking for \$SLURM_JOB_NODELIST"
	if test "x$SLURM_JOB_NODELIST" = "x" ; then 
		return 26;
	else
		return 0
	fi
}

#
# This function is called to setup 
#
scheduler_get_machinefile () {
    debug_msg " convert machine list into standard format"
    mpi_start_mktemp
    export MPI_START_MACHINEFILE=$MPI_START_TEMP_FILE
    mpi_start_mktemp
    export MPI_START_HOSTFILE=$MPI_START_TEMP_FILE
    mpi_start_mktemp
    export MPI_START_HOST_SLOTS_FILE=$MPI_START_TEMP_FILE

    sl_get_machine_list > $MPI_START_MACHINEFILE 2> /dev/null
    if test $? != 0 ; then
        scontrol show hostnames $SLURM_NODELIST > $MPI_START_MACHINEFILE 2> /dev/null
        if test $? != 0 ; then
            error_msg "Could not create machine list, both sl_get_machine_list and scontrol failed."
            return 13
        fi
    fi
    awk '!x[$0]++' $MPI_START_MACHINEFILE | tr -s " " |
         # this is a subshell, variables changes here are not changed outside!
         while read host; do
             slots=`grep -c "$host" $MPI_START_MACHINEFILE`
             echo $host >> $MPI_START_HOSTFILE
             echo $host $slots >> $MPI_START_HOST_SLOTS_FILE
         done
	export MPI_START_MACHINEFILE
    export MPI_START_HOSTFILE
    export MPI_START_HOST_SLOTS_FILE
    export MPI_START_NSLOTS=$SLURM_NPROCS
    export MPI_START_NHOSTS=$SLURM_NNODES
    export MPI_START_NSLOTS_PER_HOST=`echo $SLURM_TASK_PER_NODE | cut -f1 -d"("`

    return 0
}

#
# Most MPI implementations under slurm do not use mpirun/mpiexec
# but srun to start, this function executes the job for them
#
slurm_mpiexec() {
    MPIEXEC=srun

    mpi_start_get_plugin "generic_mpiexec.sh" 1
    . $MPI_START_PLUGIN_FILES
    generic_mpiexec
    err=$?
    return $err
}
